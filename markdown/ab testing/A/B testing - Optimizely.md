---
title: "A/B testing - The most powerful way to turn clicks into customers"
date: "2022-08-23"
urls:
  - https://enki.fra1.digitaloceanspaces.com/Learning%20TypeScript%20-%20O%27Reilly%20-%201st%20Edition.pdf
---

# A/B testing - The most powerful way to turn clicks into customers

## Chapter 2 - What to test

A mistake that some companies make is to start moving a bunch of levers around without clear planning upfront for what they’re trying to optimize—and what will be impacted by those changes. It’s tempting to just dive in and start changing parts of your homepage, or your product page, or your checkout page, without truly understanding the value that it’s generating (or not generating) for your business.

Instead, we advise a purposeful and deliberate five-step process:

- Define success
- Identify bottlenecks 
- Construct a hypothesis
- Prioritize
- Test

### Step One: Define success

Before you can determine which of your test’s variations is the
winner, you have to first decide how you’re keeping score. To start A/B testing successfully, you need to answer a specific question: __What is your website for?__ If you could make your website do one thing better, what would it do?

If the answer to that question isn’t completely clear to you, there’s a trick that might help. Imagine the following dialogue:

> ALICE: “What do you want to achieve with A/B testing?” 
>
> BOB: “We don’t know. We don’t know what we want our website to do.”
>
> ALICE: “Why don’t you take it down?”
>
> BOB: “Of course not! We need our website because it—”

And then Bob has the aha! moment that crystallizes his website’s raison d’etre: He can see reasons for the website deeper than “Everyone else has one, so we need one, too.”

Defining success in the context of A/B testing involves taking the answer to the question of your site’s ultimate purpose and turning it into something more precise: __quantifiable success metrics__. Your success metrics are the specific numbers you hope will be improved by your tests.

Part of building out your testing strategy is identifying what constitutes - and does not constitute - a "conversion" for your particular site. In online terms, __a conversion is the point at which a visitor takes the desired action on your website__. Pinpointing the specific actions you want people to take most on your site and that are most critical to your business will lead you to the tests that have an impact.

#### Macroconversions, Microconversions, and Vanity Metrics

Author and digital marketing evangelist Avinash Kaushik makes the distinction between what he calls \
- macroconversions - the metric most closely aligned with your site’s primary raison d’etre
- microconversions - the other actions that users take on your site. 

While __microconversions__ (things like clicking one button in a signup funnel, watching a video, or commenting on a blog post) may not be as immediately valuable or monetizeable as __macroconversions__, they can provide a tremen- dous amount of indirect value (provided they’re not optimized at the expense of macroconversions).

> A quick word of caution: sometimes a business can be lured into chasing “vanity metrics” that end up being distractions from the actual goal.
>
> Consider a hypothetical business-to-business (B2B) software company’s blog. The marketing team wants the blog to be a hub of thought leadership in their industry. Since they’re A/B testing the main site, they decide to start optimizing the blog, too. On the main site, their aim is clear: to use A/B testing to help drive more free trial signups. Defining quantifiable goals for the blog is harder for the team, so they have been unable to define what makes an A/B test successful.
>
> For the B2B blog, a vanity metric could be headline clicks. If this is the only piece of data you’re using to determine whether the blog is successful, you could be optimizing the wrong thing. Maybe people click headlines because they are shocking, but don’t read past them. If all you measure is clicks, you’ll never know whether the content of the actual post is good. More telling metrics might be call-to-action clicks, comments, shares, and repeat visits.

### Step Two: Identify bottlenecks

Bottlenecks are the places where your users are dropping off, or the places where you’re losing the most momentum in moving users through your desired series of actions.

### Step Three: Construct a hypothesis

Once you’ve identified what the bottlenecks are in your process, use your understanding of visitor intent to come up with test hypotheses. Consider different forms of qualitative research such as user interviews, feedback forms, or focus groups to gain an understanding of what’s going on in users’ heads as they interact with your site.

“Failed” tests are valuable because they often lead to new hypotheses for why they didn’t turn out the way you expected. Generating these hypotheses is sometimes tricky, because visitors behave in complex ways. Regardless of the complexity, however, employing the scientific method in testing will bring you closer to a meaningful understanding of your website’s audience.

### Step Four: Prioritize

Once you’ve generated hypotheses about user behavior that lead to candidate page variations for testing, you’ll need to use your intuition about what’s going to have the biggest impact to rank- order your experiments.

> “Use ROI [return on investment] to prioritize your tests,”

says Kyle Rush, who was the Deputy Director of Frontend Web Development at Obama for America. 

> “That’s one of the biggest things I’ve learned in my career.”

In a perfect world, you might test absolutely everything, but no team in the real world operates without constraints; your team’s attention, budget, time, and also your site’s traffic are all finite. These realities make prioritization of testing hypotheses a necessity.

### Step Five: Test

Once the test reaches statistical significance, you’ll have your answer.

__Often a completed test yields not only answers, but - as in any other science - more questions.__

> TL;DR
>
> - You can’t pick a winner until you decide how you’re keeping score. A/B testing starts with determining quan- tifiable success metrics.
>
> - Thereareanumberofpossibleconversiongoals:timeon site, pageviews, average order value, revenue per visitor, and so on. Take the time to pick the one that’s right for you.
>
> - Site analytics along with your own instincts will suggest bottlenecks where you can focus your attention.
>
> - Understanding visitor intent with the help of interviews and usability tests will suggest hypotheses about what to change and how.
>
> - Prioritize your experiments based on your prediction of their impact.
>
> - Start testing, and continue until you begin to see dimin- ishing returns.

---

## Chapter 3 - Seek the global maximum

While refinement can lead to a solution better than what you have today, we recommend exploring multiple alternatives that might not resemble the current site first. We encourage the kind of humility and bravery required to say, 

> “You know, the website we have today is far from perfect. Let’s try some dramatically new layouts, new designs, and redesigns, figure out which of those work well, and then refine from there.”

However, it’s not as simple as saying that one should always explore first and always refine second. The truth is that exploration and refinement are complementary techniques, and most effective when used in tandem.

---

> Break from the Status Quo: ABC Family
>
> Disney ran an experiment using Optimizely on the ABC Family homepage.
> The page displayed a large promotion for a television show you might be interested in. After looking through their search logs, however, the Disney digital team discovered that a lot of people were searching for the exact titles of shows and specific episodes. Instead of taking the incremental approach (e.g., by tweaking the promo image, or rotating the featured show), the team decided to reevaluate their entire approach. They created an alternative view, one that was less visual and more hierarchical, in which users can drill down through menus to specific shows.
>
> Disney had defined as their quantifiable success metric the percentage of visitors who clicked on any part of the experiment page. Their goal was to lift this engagement by 10 to 20 percent. In fact, by being open to this big, fundamental change, they were able to effect an engagement increase of more than 600 percent.

---

> Learn Your Way to the New Site: Chrome Industries
>
> Kyle Duford at cycling bag and apparel manufacturer Chrome Industries explains that the Chrome team is presently discussing a major site redesign. “We’re purposely using all of these tests to formulate how we approach the new website.”
>
> The Chrome team discovered something surprising when they were A/B testing the order of the three promotional content blocks on their homepage: the content they put in the center block seemed always to outperform the content they put in the left block.
> 
> The team’s assumption was that because people read from left to right, they would explore in this manner. “This is gold,” says Duford. Now they know to put their most important promo block in the center, but the bigger lesson is that users seem to go straight for the central imagery, rather than scanning left to right. 
>
> This is a valuable insight that may end up altering the entire new layout for the site redesign. “The look and feel will be completely different, but the ideas of the blocks of content that go into it are all being discovered through this process,” Duford says. “So while it’s important right now to understand how people shop, it’s more important because it’s going to inform our decisions going forward.”

---

> Rethink the Business Model: Lumosity
>
> Lumosity is a company that offers web-based games designed to improve users’ minds. Their business model is simple: users pay a monthly subscription fee to access games designed by neuroscientists to promote cognitive function. Users derive the most benefit from training regularly, and boosting user engagement was an important goal. What wasn’t intuitive, however, was what the Lumosity development team did to increase this metric.
>
> Lumosity’s scientists recommended that users train for 15 to 20 minutes a day, 4 to 5 times per week - not unlike physical exercise -although the site didn’t actually constrain users to a specific time limit. The data showed that people would stay logged in for many hours, but that over time, the frequency of logins declined, suggesting users were burning out.
>
> The team hypothesized that limiting the amount of training a user could do in one day would improve engagement over time.
>
> Giving users one training session a day and congratulating them on being done for the day might achieve their goal. Such a radical change initially made many people at the company nervous, including Product Manager Eric Dorf, who feared that restricting the amount of a time a user could use the service they were paying for would frustrate the user base. 
>
> “I felt like, if I’m paying for this as a subscription and I’m not allowed to train as much as I want, why would I pay for it?” he says. “I remember thinking, ‘Gosh, I hope we don’t piss everybody off.’”
>
> Trying out the new model as part of an A/B test mitigated that risk. The team ran an A/B test that set the original, unlimited training against the limited-training variation. The results shocked Eric and his team. Users actually trained more over time in the new model. “The graph was so clear,” Eric says. “People were training more as a result of being limited.”
>
> After making this discovery, the Lumosity team changed the way they position, build, and sell their program. The message of daily training is the cornerstone of their communications to users. After this initial exploration, the team then subsequently used A/B testing to refine the approach, finding the messages and marketing that best support and reinforce the idea of daily training.
>
> Today, when a user completes a session, the message is, “You’re done. You can leave the site now,” Dorf explains. “It’s not like a lot of other gaming products that want you to spend all your time playing. The scientists are happy because more users are more engaged with training than before.”

---

> Test Through the Redesign, Not After: Digg and Netflix
>
> ---
>
> When it comes to making better data-driven decisions, the sooner the better. Often the temptation is (and we’ve heard this before) “Oh, we’re doing a redesign; we’ll do the A/B testing afterwards.” The fact is you actually want to A/B test the redesign.
>
> Around 2010, we were introduced to the folks at Digg by their new VP of Product Keval Desai to talk about using Optimizely. Their response was, “We are busy working on a complete overhaul of our site. After we do that, then we’ll do A/B testing.”
>
> As Desai explains, the “Digg v4” redesign was a perfect storm of problems. The company rolled out a new backend and a new frontend at the same time, conflating two different sets of challenges. “It was a big bang launch,” he says. The backend couldn’t initially handle the site traffic and buckled on launch day. What’s more, despite faring well in usability tests, focus groups, surveys, and a private beta, the new frontend met with vociferous criticism when it was released to the public, and became a magnet for negative media attention. 
>
> “When you change something, people are going to have a reaction,” Desai says. “Most of the changes, I would say, were done for the right reasons, and I think that eventually the community settled down despite the initial uproar.” But, he says, “a big-bang launch in today’s era of continuous development is just a bad idea.” “To me, that’s the power of A/B testing: that you can make this big bet but reduce the risk out of it as much as possible by incrementally testing each new feature,” Desai explains. People are naturally resistant to change, so almost any major site redesign is guaranteed to get user pushback. The difference is that A/B testing the new design should reveal whether it’s actually hurting or helping the core success metrics of the site. “You can’t [always] prevent the user backlash. But you can know you did the right thing.”
>
> ---
>
> Netflix offers a similar story of a rocky redesign, but with a crucial difference: they were A/B testing the new layout, and had the numbers to stand tall against user flak. 
> 
> In June 2011, Netflix announced a new “look and feel” to the Watch Instantly web interface. “Starting today,” wrote Director of Product Manage- ment Michael Spiegelman on the company’s blog, “most members who watch instantly will see a new interface that provides more focus on the TV shows and movies streaming from Netflix.”
>
> At the time of writing, the most liked comment under the short post reads, “New Netflix interface is complete crap,” followed by a litany of similarly critical comments. The interface Netflix released to its 24 million members on that day is the same design you see today on netflix.com: personalized scrollable rows of titles that Netflix has calculated you will like best. So, in the face of some bad press on the blogosphere, why did Netflix decide to keep the new design? The answer is clear to Netflix Manager of Experimentation Bryan Gumm, who worked on that redesign: the data simply said so.
>
> The team began working on the interface redesign in Janu- ary 2011. They called the project “Density,” because the new design’s goal was literally a denser user experience.
>
> The original experience had given the user four titles in a row from which to choose, with a “play” button and star rating under each title’s thumbnail. Each title also had ample whitespace surrounding it a waste of screen real estate, in the team’s opinion.
>
> The variation presented scrollable rows with title thumbnails. The designers removed the star rating and play button from the default view, and made it a hover experience instead.
>
> They then A/B tested both variations on a small subset of new and existing members while measuring retention and engagement in both variations. The result: retention in the variation increased by 20 to 55 basis points, and engagement grew by 30 to 140 basis points.
>
> The data clearly told the designers that new and existing members preferred the variation to the original. Netflix counted it as a success and rolled the new “density” interface out to 100 percent of its users in June 2011. As Gumm asserts, “If [the results hadn’t been] positive, we wouldn’t have rolled it out.” The company measured engagement and retention again in the rollout as a gut-check. Sure enough, the results of the second test concurred with the first that users watched more movies and TV shows with the new interface.
>
> Then the comment backlash started.
>
> However, as far as Netflix is concerned, the metrics reflecting data from existing and new members tell the absolute truth. As Gumm explains, the vocal minority made up a small fraction of the user base and they voiced an opinion that went against all the data Netflix had about the experience. Gumm points out, “We were looking at the metrics and people were watching more, they liked it better, and they were more engaged in the service.
>
> ... [Both the tests] proved it.”
>
> Gumm also makes the following very important point: “What people say and what they do are rarely the same. We’re not going to tailor the product experience, just like we’re not going to have 80 million different engineering paths, just to please half a percent of the people. It’s just not worth the support required.”
>
> Gumm then reminds us that despite the few loud, unhappy customers that may emerge, the most critical thing to remember is the data: “I think it’s really important in an A/B testing organization, or any data-driven organization, to just hold true to the philosophy that the data is what matters.

> TL;DR
> - Incrementalism can lead to local maxima. Be willing to explore to find the big wins before testing smaller changes and tweaks.
> - Conversely, sometimes it’s the incremental refinements that prove or disprove your hypotheses about what your users respond to. Use the insights from small tests to guide and inform your thinking about bigger changes.
> - Consider entirely new alternative approaches to your principal business goals. Be willing to go beyond just testing “variations on a theme”—you might be surprised.
> - If you’re working on a major site redesign or overhaul, don’t wait until the new design is live to A/B test it. A/B test the redesign itself.

---

## Chapter 4 - Less Is More: Reduce Choices - When Subtraction Adds Value

Sometimes the winning variation of a page is one in which you haven’t added anything at all but in fact removed elements from the page. We have seen many teams improve conversion metrics simply by adhering to the design mantra “Less is more.” The products of this approach - simpler pages, shorter forms, and fewer choices—can make a very big difference.

### Every Field Counts: The Clinton Bush Haiti Fund

When you’re asking the user to take an action, every bit of effort counts, and so we wanted to look at the form to see if there was any way we could streamline the user’s experience. 

> We noticed that the Foundation had included fields for “phone number” and “title,” hoping down the road to be able to use this information, possibly to make phone solicitations. 
>
> The fact was, however, that the Foundation was stretched so thin that it wasn’t actually calling anyone, so this additional information being requested of users wasn’t being put to use. 
>
> We hypothesized that getting rid of these two optional fields, even if it came at the cost of some potentially useful data, would be more than made up for by added donations in virtue of the simpler form.
>
> The effect was instantly measurable and dramatic. Simply removing two optional fields resulted in an 11 percent improvement in dollars per pageview over the length of the test - a massive gain in donations from a small simplification.

### Keep It Simple: SeeClickFix

> SeeClickFix is a web tool that allows citizens to “report neighborhood issues, and see them get fixed.” The tool centers on a web-based map that displays user activity. Users add comments, suggest resolutions, and add video and picture documentation. Anyone can elect to receive email alerts based on “Watch Areas” by geographical area and can filter reports by keyword.
>
> The original SeeClickFix homepage contained a simple call to action with one form and a simple design. After a great deal of work by the team’s designers and engineers, SeeClickFix had a brand-new homepage ready to launch, complete with an interactive map. 
>
> The team was excited about it, and used an A/B test to find out just how brilliant their new design idea was.
>
> They were in for a surprise. SeeClickFix actually drove 8 percent more engagement on the simple gray box form that displayed a simple call to action and a description. The proposed new homepage may have been more technologically sophisticated and visually rich, but simplicity mattered where it counted most: getting visitors to engage with the site.

### Hide Options: Cost Plus World Market

The checkout funnel is a prime place for optimization on a site, and a place where it’s often true that less is more. 

This makes intuitive sense: nonessential steps included in the purchase process can be distracting, and it’s no surprise that minimizing obstacles frequently boosts conversion. What’s interesting is that removing unnecessary options can also reduce the overall friction of the process in a significant way.

> Cost Plus World Market, a chain of specialty/import retail stores and a subsidiary of Bed Bath & Beyond, ran an experiment that tested hiding the promotion code and shipping options form fields from the last page in the checkout funnel.
>
> By hiding these two optional fields and making them expandable links instead, Cost Plus saw a 15.6 percent increase in revenue per visitor. Conversions also went up by 5.2 percent in the variation with hidden fields.

### Remove Distractions and “Outs”: Avalanche Technology Group

It’s important to remember that any clickable element on a page represents an option for your user, not just those explicitly included in the checkout process. Our next example illustrates how valuable it can be to focus on those elements that aren’t actually part of the checkout process.

> Avalanche Technology Group is the Australian distributor for popular antivirus software AVG. When they examined their shopping cart conversion data they suspected there was room for improvement, and wanted to experiment with some “minor” (or so they thought) variations that would leave the actual steps of the checkout process untouched.
>
> The team decided to run an experiment in which the site’s header navigation links were removed from the checkout funnel, which they hypothesized would reduce visual noise and keep traffic more focused on actually checking out.
>
> This change alone improved conversion rates by 10 percent and led to a 16 percent increase in revenue per visitor, showing that even visually minor changes to the “auxiliary” parts of a page can have a big impact on visitor behavior.

### Lower the Slope: Obama 2012

> The previous examples show the potential benefits of keeping things simple by removing distractions and minimizing options. But how do you optimize when things are already as simple as they can possibly be? Or are they?
>
> Every one of the 165 people on Obama’s 2012 digital team understood how mission-critical A/B testing was to running the digital campaign. “We didn’t have to convince anybody that A/B testing was important; it was just a no-brainer,” recalls Kyle Rush, one of the lead developers on the campaign, who was responsible for much of the testing program.
>
> The Obama 2012 team executed nearly 500 A/B tests over a 20-month period. The team experimented with everything from imagery and design to copy and usability. As a result, the optimization program collectively brought in an extra $190 million in campaign donations. One of the key tests that contributed to the $190 million pot was what came to be known as “Sequential.”
>
> The original donation process was a single page with a form and a picture of the president playing basketball. This page was already highly optimized: the image had been tested and not one superfluous form field existed - only the legally required ones remained. It looked pretty, and it was converting, but the campaign team wanted to see if they could go further. 
>
> Since federal law requires specific information from campaign donors, the team couldn’t just eliminate form fields at will. On the other hand, they knew from usability tests that the form was too long and losing potential donations. What to do? The team had an idea: make the form appear shorter by breaking it into pieces.
>
> Once the form was divided into a sequence, the next logical thing to test was the order of the sequence. “Asking for the donation amount first more closely matches the users’ state of mind,” Rush explains. “Once they’ve made the decision to donate they’re ready to enter an amount, not their personal information.” Optimizations to the sequencing confirmed that donation amount should come first, then personal information, then billing, and occupation/employer last.
>
> The optimized form yielded a 5 percent conversion increase over what had initially seemed to be the maximally optimized page. As Rush puts it: “You can get more users to the top of the mountain if you show them a gradual incline instead of a steep slope.”

## Chapter 5 - Words Matter: Focus on Your Call to Action - How a Few Words Can Make a Huge Difference

__The wording on your site represents an area where there are virtually inexhaustible opportunities for experimenting with variations.__ The variations can be crafted with just a few keystrokes, and often the slightest change can have a major effect. 

Because language is so easily tweaked (compared to art or images, which require careful design work) and its possibilities are so vast, it represents a major opportunity to test variations at the speed of brainstorming itself. The words on a site are some of the most powerful and potent elements a user sees. The right combination can be leaps-and-bounds more effective than the rest.

### Reconsider “Submit”: The Clinton Bush Haiti Fund

> Instead we tried the label “Support Haiti,” hypothesizing that making the button reflect the purpose of their action would make the meaning of their clicks more immediately clear to users.
>
> The difference was enormous. The effect of the change from “Submit” to “Support Haiti” was on the order of several dollars per pageview, and this small change, together with our optimizations to the form and several other quick, simple tests, managed to bring in an additional million dollars of relief aid to Haiti. It’s a testament to the power just one or two words can have.

### Find the Perfect Appeal: Wikipedia

> For a big fundraising push, Wikipedia’s team comes up with dozens of variations to test. Finding the most effective words among the essentially endless options is a huge task—one that takes a lot of creativity and a lot testing. “You have to have a rule that if anybody feels strongly about testing something, you test it,” Exley says.
>
> There’s one test in particular that stands out in Exley’s mind. There was a fundraising appeal that had done well as part of the site’s landing pages:
>
> “If everyone reading this donated $5, we would only have to fundraise for one day a year. Please donate to keep Wikipedia free.” One of the Wikimedia team members suggested testing what would happen if they replaced the last third of their fundraising banner with this line, and Exley agreed to a test.
>
> This variation was a bit of a gambit, because setting the bar so distinctly at $5 had the potential to “anchor” users’ minds at a lower level than the one suggesting “$5, $20, $50, or whatever you can.” On the other hand, the logic of the appeal - and perhaps the very absence of higher dollar values - might persuade more users to give.
>
> The outcome: even though the five-dollar appeal lowered the average donation amount by 29 percent, the rate of donation went up by a whopping 80 percent, resulting in a net increase in overall amount raised of 28 percent.

### Why versus How: Formstack

Deciding what to include in a site’s main navigation, and how to arrange it, is key to establishing the flow of traffic and the focus of the site - and it can provoke strong differences of opinion.

> While redesigning their site, the team at online form builder Formstack sat around a table, considering their navigation. They all agreed on highlighting the form types, features, examples, and pricing, but were not sure what would be the best page to use as a lead.
>
> The team settled on using "Why Use Us" as the lead navigation item because they suspected it would help persuade visitors that Formstack is a better choice than the competition. As analytics on the new site design filtered in, they noticed that visitors weren’t clicking on “Why Use Us” as much as they had expected. 
>
> That prompted a follow-up test: they tested whether naming the page "How It Works" would prompt more visits.
>
> Although “Why Use Us?” was the question the Formstack team ultimately wanted to answer for their visitors, they decided to try the header “How It Works” because, their thinking went, it would invite visitors to investigate on their own without the obvious self-promotion.
>
> “How It Works” also helps a user unfamiliar with web form builders get his or her bearings on what it is that Formstack does as a company, whereas “Why Use Us” might suggest an explanation of how Formstack differs from its competitors, rather than what its product does in the first place.
>
> In an A/B test pitting “Why Use Us” against “How It Works,” the winner was clear. Naming the lead navigation item “How It Works” increased traffic to that page by nearly 50 percent, and also lifted two-week free-trial signups by 8 percent.
>
> “Instead of getting bogged down in disagreements, we moved forward,” says Jeff Blettner, a web designer at Formstack. “We knew that we would be able to come back after launch and test our hypotheses.”

### Nouns versus Verbs: LiveChat

> LiveChat sells software that allows businesses to talk with their website visitors in real time.
>
> In order to figure out how to maximize the company’s product sales, LiveChat visual designer Lucy Frank evaluated the steps most people take in signing up for the service.
>
> She found that most visitors sign up for a free trial before becoming paying customers, and so she hypothesized that increasing the number of people in free trials might result in more sales downstream.
>
> Since the first step to starting a free trial is clicking the big shiny button on the homepage, Frank began her experimentation there. She decided to simply change the call-to-action text on the button from “Free Trial” to “Try it free,” and see which version enticed more users to register.
>
> The team hadn’t expected to see much of a variation in terms of results from making such seemingly small changes. Yet the difference of just two words increased click-through rate by 14.6 percent.
>
> This experiment is a great example of what we’ve seen again and again across a wide range of businesses. We usually give folks some pretty straightforward advice when they ask about how to improve their calls to action: verbs over nouns. In other words, if you want somebody to do something, tell them to do it.

### Framing Effects

There are endless possibilities for any call to action, and it’s not feasible to test them all. So, how do you focus your tests on only the possible alternatives that are most likely to have an impact? Having a good hypothesis of why a change will be effective is a crucial step, and one powerful theory to help you formulate it is called __framing__.

Framing is the simple idea that different ways of presenting the same information will evoke different emotional reactions, and thus influence a person’s decision. For example, as Nobel laureate psychologist Daniel Kahneman notes in Thinking, Fast and Slow:

> The statement that “the odds of survival one month after surgery are 90%” is more reassuring than the equivalent statement that “mortality within one month of surgery is 10%.” Similarly, cold cuts described as “90% fat-free” are more attractive than when they are described as “10% fat.”

> A famous 1995 study by psychologist Sara Banks et al. involved showing two groups of women videos on breast cancer and mammography in an attempt to convince them to get screened. 
>
> The first group’s video focused on gains, that is, it espoused the benefits of having a mammogram.
>
> The second group’s video was loss-framed: it emphasized the risks of not having one. Though the two videos presented the same information, only 51.5 percent of those who saw the gain-framed video got a mammogram in the next year, whereas 61.2 percent of those who saw the loss-framed video did so.

__There are no one-size-fits-all rules about message framing__, and it’s still important to test variations. However, an awareness of framing helps to define the scope of possibilities.

You might, for instance, choose to avoid testing more equivalent phrases and consider strikingly different ways you can frame your value proposition and test each of them.

Try asking yourself:
- Is the language negative or positive? Do you, for instance, advertise what a product has or what it doesn’t have?
- Is the language loss-framed or gain-framed (e.g., the mammography study)?
- Is the language passive or action-oriented (e.g., LiveChat’s “Try it free” button)?

> TL;DR
> - There are endless word combinations to use on your website. Don’t be afraid to brainstorm and think broadly: a testing platform lowers the “barrier to entry” of ideas, minimizes the risks of failure, and enables quick and potentially huge wins.
> - Decisions around messaging and verbiage can easily lead to contentious debates on a team. A/B testing is a way to put opinions aside and get concrete feedback on what works and what doesn’t. (We’ll revisit this idea in Chapter 8.)
> - If you want someone to do something, tell them to do it.
> - Different ways of framing the same message can cause people to think of it in different ways. Play with alternative ways of framing the same information and see what differences emerge.

---

## Chapter 6 - Fail Fast and Learn - Learning to Embrace the Times When A Beats B

Even “failed” experiments have their silver linings: recognizing that a particular change will harm your goals is inarguably better than simply making that change, and as an added benefit experiments like this are often the ones that teach us the most about our visitors and what drives them.

### Prime Real Estate versus Familiar Real Estate: IGN

> Gaming website IGN wanted to encourage more visitors to the video site that brings them a big portion of their ad revenue. So they tried running an A/B test where they moved the “Videos” link over to the left of the main navigation.
>
> There are plenty of organizations in which a change like this would have come down the chain of command once-and-for-all from the HiPPO - the Highest Paid Person’s Opinion. But before making the change for good, IGN ran an experiment to see exactly how much of an increase they could expect to see from giving the “Video” link top billing.
>
> Not only did the test, shockingly, show no increase at all, it showed that the new banner dramatically reduced the video click rate by 92.3 percent. If they had blindly moved the “Videos” link without testing it first, the change could have been disastrous. Because IGN gets so much traffic, it only took them a matter of hours to get statistically significant results. They were able to cease the experiment, return to the original design, and go back to the data for more answers.
>
> The test saved IGN from a potential catastrophe that would have occurred had they simply rolled out the new navigation, but there’s a bigger lesson. One of the biggest reasons for dramatic results like this one is that a lot of a site’s traffic typically comes from returning visitors, users who are accustomed to seeing the site in a certain way - in this case, with the “Videos” link on the far-right side, not the far-left. 
>
> When it’s missing from the spot they normally go to find it, they’re not going to do the work to locate it. 
>
> Considering the root cause of the results offers lessons not only about proposed changes but, at a deeper level, about the testing process as well. Moving forward, the team can consider the fact that new and returning users are going to have very different experiences of the site. Keeping this in mind will bear fruit in subsequent tests.

### What’s Good at One Scale Isn’t Always Good at Another: E-Commerce

Many retailers have customer reviews and star ratings displayed on their sites. 

> One large online retailer discovered through testing that displaying the rating prominently on individual product pages helped conversion.
>
> So the e-commerce team there experimented with adding the ratings to the category page — the page one level up from the product page that shows all of the items in a category.
>
> It seemed like common sense to the e-commerce team: showing the stars on the spill page should motivate people to click through and view the products more often thereby increasing conversions. 
>
> Good thing they tested it because that was not the case, the variation did more harm than good. As it happened, showing star reviews made customers convert 10 percent less. The test illustrates that what works at one level of scale doesn’t always work at another level; what was good for the product page ended up being bad for the category page. 
>
> It’s a good reminder that just because something makes sense on a particular part of your site - or is even proved to be advantageous - doesn’t mean you should roll it out to other parts of the site without checking first.

### What Buyers Want Isn’t Always What Sellers Want: Etsy

> Handmade- and vintage-goods marketplace Etsy has over 42 million unique visitors per month and is among the Alexa Top 200 websites. A/B testing is an important means for the product developers and engineers to collect behavioral data about how Etsy’s 800,000 sellers and 20 million members use the site.
>
> Etsy users are shown an activity feed in which they can see highlights from fellow Etsy members they follow: the items those people are favoriting and purchasing. There are thousands of items posted to Etsy weekly, and this activity feed is a handy way for users to discover new items on the site.
>
> The activity feed displays a combination of activities happening for buyers and sellers in one list. In what the team thought would be a much improved experience, they redesigned the feed and removed the “Your Shop” view from it, leaving only the “People You Follow” view. They A/B tested the original feed against the redesigned one to see how the redesign fared with users.
>
> To the team’s surprise, engagement with the feed dramatically decreased in the variation. After a closer look at the data, they discovered a certain type of use case that the team didn’t anticipate. 
>
> It turned out that sellers were using their own activity feed to manage their shops: as a timeline of what items they had listed at what times. The team envisioned the feed as a tool for buyers to scroll through what people were doing on the site.
>
> But sellers had been using this to manage their shops and the new “reskinning” removed this functionality for them.
>
> Without this surprise result, the Etsy team would never have known about this use case. Now, not only could they take it into account during the redesign, they could actually design for it.
>
> Their next iteration included two buttons: one called “Following,” and another for “Your Shop”. The story ends happily with Etsy now actively building site functionality around a usage that, until their original redesign hit a snag, they had never known about.

### When a Win Isn’t a Win: Chrome Industries

> The Chrome e-commerce team has experimented with a plethora of image treatments for their urban biking products over the years, and recently decided to test whether a product video spurred more visitors to make purchases than did a static image.
>
> The objective of the test was to determine whether to commit more resources toward video development. The team picked one product to experiment with: their Truk shoe.
>
> They measured the percentage of visitors to the Truk product page from the category page, the percentage of visitors who continued to checkout, and percentage of visitors who successfully ordered.
>
> After letting the test run for just under three months, the results were something of a wash. Users visited the Truk product page 0.5 percent more with the image, continued to checkout 0.3 percent more with the video, and successfully ordered 0.2 percent more with the video.
>
> If anything, the video slightly edged out the static image, but because producing video involves a much higher investment from Chrome than the images, the verdict is actually a clear vote against the added production cost.
>
> Chrome can table the issue for the time being, or it can further investigate the reason why video didn’t convert, rather than moving forward under the assumption that video will drive sales and ramping up a full-blown video asset initiative that won’t necessarily prove its return on investment.
>
>If the team does choose to test video down the line (e.g., in seeing how lifestyle-oriented video might compare against product-oriented video), they can at least be confident that there’s little risk in running the follow-up test, since they’ve proven that video won’t hurt conversion.
>
> They may also decide simply to allocate their energies elsewhere and experiment with optimizing different portions of the site entirely, where there may be bigger unrealized gains awaiting.

---

> TL;DR
> - What works for returning users may not work for new
users, and vice-versa.
> - Something that works on one page may not work on another; something that works at one scale may not work at another.
> - What one type of user wants may not be what another type of user wants. A failed test, sometimes more than a successful test, may prompt a drill-down that reveals a key difference between segments of users.
> - Sometimes a variation may win against the original, but it may not win by enough of a margin to justify the implementation overhead or other drawbacks of the vari- ation that are external to the test itself.
> - Any test that reveals that an initiative isn’t performing is a blessing in disguise: it allows you to free up resources from things that aren’t working and divert them to the things that are.

---

## Choose the Solution That’s Right for Your Organization - Deciding Whether to Build, Buy, or Hire

The first step is making a high-level decision about how you’ll bring testing onboard: you can build your own testing tool in-house, buy a testing platform, or hire a consultant or agency to do the testing for you. There’s no wrong choice, and each has its pros and cons. Indeed, many organizations elect to combine more than one of these approaches. We walk you through the things to consider when making the decision about what’s best for your needs.

### Option One: Build

Building a testing solution in-house is a viable option for organizations that have significant engineering resources available. We’ve found that most companies don’t decide to suddenly build a testing tool from scratch without an engineering team that’s closely tied to the process. A homegrown testing tool is usually something that organizations add on to an already established data-gathering and analytics machine.

Building an in-house solution requires substantial engineering effort, so it’s rare for small companies with limited engineering resources to pursue this path. Typically only larger teams with specialized needs and enough dedicated engineering resources to pull it off will build a solution for themselves. 

> For example, Amazon has invested considerable effort over many years to build an extensive testing platform that is integrated closely with their website, product catalogue, and customer database.

There are many reasons why a company might choose to invest in a homegrown testing tool, but the biggest is probably the desire to run experiments that require a deep connection with proprietary internal systems, like Amazon’s customer database in the example above. 

Custom-built testing platforms can provide specialized experiment targeting capabilities, tight integration with your build/deploy systems, and the ability to experiment with complex, server-side logic, like a search ranking algorithm.

Sometimes the decision to build an A/B testing capability to the website arises out of iterative additions to an in-house analytics platform. 

> This was the case for Etsy, an e-commerce site that sells handmade and vintage goods. Today, their team runs every change or new feature release through an A/B test, but they didn’t always work that way.
> 
> Dan McKinley started at Etsy in 2007, and discusses how they didn’t do any measuring or A/B testing in the early days. “Nor did I really have any conception that we should have been doing it,” he confesses. By 2011, McKinley had noticed a pattern in the way they developed products. 
>
> As he describes it, the engineers would spend a great deal of time and effort working on a new feature up front. They would release that feature, and then talk about it at a company-wide meeting where there would be a lot of applause for the new feature. Then they would move on. 
>
> Two years later, they would eliminate the feature they had spent all their time and effort developing—because it turned out that nobody was using it. “I realized that we were failing in what we were trying to do; we just weren’t very good at realizing that we were failing,” McKinley told us. “[We wanted] to be better at realizing we were failing, and if at all possible, not fail. And that was the motivating factor in my getting into experimentation on the web.”
>
> Etsy has had access to a lot of data, data engineer Steve Mardenfeld explains, pretty much since the site launched. In turn, they have tools to collect, examine, and analyze that data in an effort to improve the user experience on the site. For example, examining the data lets them improve their search algorithms and recommendations. Whenever the engineers created a new feature, they would release it to a small percentage of users and look for any operational concerns. Once they knew it was functioning well (i.e., it wouldn’t break the site) they would release it to 100 percent of Etsy users.
>
> “So we were already doing the basic idea of A/B testing; we just weren’t actually measuring anything,” Mardenfeld says. “It just seemed like a really good fit to try to shoehorn this into the process that we already had.”
>
> Etsy’s team of 100þ engineers decided to go with an in-house testing tool because they already had the infrastructure in place to support A/B testing. All they had to do differently was start tracking how the new experience performed against the original experience.

#### The A/A Test

When building your own A/B testing tool from scratch, one of the obvious areas of concern will be simply making sure that the tool is functioning accurately.

One of the handiest ways to verify the accuracy of a testing tool is an A/A test. An A/A test, as the name implies, involves testing two identical versions of your page to ensure there are no statistically significant differences reported between them. If there are, then something fishy or erroneous may be happening with the way the test is being run or the way the data is collected or analyzed. An A/A test is a good way to assure yourself (and your boss) that your testing platform is functioning correctly.

### Option two: Buy

Let’s talk about what you get when you buy a testing platform. Most follow the Software-as-a-Service (SaaS) model; in other words, you won’t download anything or purchase a physical product. Rather, integration happens as easily as a one-time copy-and-paste onto your site, after which you access the software through the web and your tests and data live in the cloud. Buying a testing platform makes sense for a range of group sizes - individuals, small companies, and large companies.

At Optimizely we work with companies ranging from self-service startups to Fortune 100s that are equipped with large testing teams. SaaS solutions offer a number of advantages:

- __Built-in features__: An obvious advantage of buying a testing solution is that advanced testing features are included in your purchase. (You can, for example, target visitors from Facebook who see one variation, and compare them to Pinterest visitors who see a different one).
Commercial testing software is typically purchased as a subscription and many platforms offer multiple subscription tiers, with additional built-in features available in higher tiers.

- __Automatic updates__: When building a homegrown testing platform, the ability to have total control over the platform requires an ongoing engineering commitment: everything the company wants the testing suite to incorporate requires engineers to build, test, and maintain it over time.
A company using an off-the-shelf SaaS product will effectively remain at the cutting edge without additional effort.

- __WYSIWYG editing__: Leading A/B testing SaaS platforms enable marketers, advertisers, designers, and other non- technical users to easily create and test page variations, using a visual what-you-see-is-what-you-get (WYSIWYG) editor that doesn’t require writing any custom code.

- __Trustworthy reporting__: Accurate and reliable statistical reporting and calibration are essential for any data-driven organization. When you purchase an off-the-shelf testing solution, you’re buying something teams have spent time building, optimizing, and debugging. You can therefore trust them to give you accurate results about how your tests performed. What’s more, these platforms are constantly being tested by the thousands of clients using them. (If you want to test the tool’s accuracy yourself, you can of course run an “A/A test” as discussed above.)

- __Professional Support__: Most A/B testing platforms offer some form of dedicated technical support, and some even offer additional consulting services (for an additional charge). Technical support is especially important for teams in which non-technical users are driving the testing process.

- __Community__: When you sign up for an A/B testing platform, you are joining an existing community of users who are available to answer questions, give technical support, and suggest best practices.

---

Questions to Consider When Evaluating an A/B Testing SaaS Solution:

1. __Does the platform integrate with other tools you already use?__
    
    For most businesses, an A/B testing tool will complement the other tools you’re already using, especially analytics.
  
    The more your efficiency, data collection, and lead-generation/nurture tools communicate with each other, the more effectively you can use them in concert. Plus, integrating all of these can maximize the ROI you get out of each service alone.
2. __Does it meet your budget?__

    Testing solutions charge in a variety of different ways and while we encourage the reader to explore her options, ultimately the decision should come down to ROI: will the gains achieved by regular testing outweigh the usage fees paid for the platform? 
    
    If budget is a concern, it may be possible to start small and expand: for example, for companies with multiple web properties or an international presence it might make sense to start with a single property/region and expand as you begin to realize gains.

3. __How well do you gel with the platform provider’s team and support approach?__

    Because many tools can provide similar features technologically, the brand’s personality, dedication to customer success, and availability are other critical elements to consider. Support on the platform must come from the platform provider, so ensuring the help is there if you need it is important. 
    
    It may indeed be the variable that helps make your decision, since what differentiates one testing tool from the next is, in large part, the people. Are you looking for a team of people that can come up with great testing ideas for you? If you’re new to A/B testing, you might need a company that has dedicated support resources available 24/7 should you have questions.

### Option Three: Hire

The final option is to hire an agency or an optimization consultant to do testing for you.

An agency is a service independent from the client that provides an outside point of view on how best to sell the client’s products or services—or in this case, optimize the client’s website.

Most digital marketing agencies are quickly adding A/B testing to the list of services they offer; they’re also partnering with testing platforms in order to use them on clients’ websites. Companies can hire agencies or consultants as short- or long-term solutions for testing.

There are myriad reasons to outsource testing. For instance, a company that doesn’t have the internal resources to allocate to testing will instead choose to hire another entity to take care of all strategy and testing implementation. 

In another scenario, a company might have the bandwidth for ideating tests but lack the technical know-how to execute them. In this case, they’d work with an agency to implement tests. The reverse is also common, that is, for a company to purchase a testing platform and work with a strategic consultant to come up with test ideas.

If you outsource any part of testing — either the creative or the actual execution — then there are a few things to look for before you sign a contract. 

You want to make sure that the third party has a good track record with optimization strategy and implementation. They should be experts in each testing platform they offer, and provide technical support should you need it. 

With many agencies you’ll pay the agency per-hour or per-experiment, and some also offer “unlimited” or “constant” testing programs: for a fixed price they conduct an unlimited number of tests within an overall program. Compare plans and think about what will work best for your needs before signing on.

It comes down to a tradeoff between investing time and training in building a team internally, or investing money in an agency. If you’re not ready to build an internal testing team who will use a homegrown platform or an SaaS, then your best option is to hire an outside service to handle your testing.

---

> TL;DR
> - Building your own testing platform requires a significant and ongoing engineering investment, but can ultimately provide the greatest level of control and the tightest integration with your team and deployment cycles.
> - An A/A test is a helpful way to ensure that your solution is functioning, reporting, and analyzing correctly.
> - Many A/B testing Software-as-a-Service (SaaS) platforms are easy to use without requiring engineering support: marketers and product people without a coding background can create and run variations in a visual WYSIWYG environment.
> - An agency can help your team with the ideation of tests, execution of tests, or both.
> - When picking the solution that best fits your company, consider making key stakeholders part of the exploration process. The earlier you bring others on board, the easier it will be to get buy-in.

---

## Chapter 8 - The Cure for the HiPPO Syndrome - Getting Buy-In and Proving Value

