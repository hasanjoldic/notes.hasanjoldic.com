---
title: "A/B testing - The most powerful way to turn clicks into customers"
date: "2022-08-23"
urls:
  - https://enki.fra1.digitaloceanspaces.com/Learning%20TypeScript%20-%20O%27Reilly%20-%201st%20Edition.pdf
---

# A/B testing - The most powerful way to turn clicks into customers

## Chapter 2 - What to test

A mistake that some companies make is to start moving a bunch of levers around without clear planning upfront for what they’re trying to optimize—and what will be impacted by those changes. It’s tempting to just dive in and start changing parts of your homepage, or your product page, or your checkout page, without truly understanding the value that it’s generating (or not generating) for your business.

Instead, we advise a purposeful and deliberate five-step process:

- Define success
- Identify bottlenecks 
- Construct a hypothesis
- Prioritize
- Test

### Step One: Define success

Before you can determine which of your test’s variations is the
winner, you have to first decide how you’re keeping score. To start A/B testing successfully, you need to answer a specific question: __What is your website for?__ If you could make your website do one thing better, what would it do?

If the answer to that question isn’t completely clear to you, there’s a trick that might help. Imagine the following dialogue:

> ALICE: “What do you want to achieve with A/B testing?” 
>
> BOB: “We don’t know. We don’t know what we want our website to do.”
>
> ALICE: “Why don’t you take it down?”
>
> BOB: “Of course not! We need our website because it—”

And then Bob has the aha! moment that crystallizes his website’s raison d’etre: He can see reasons for the website deeper than “Everyone else has one, so we need one, too.”

Defining success in the context of A/B testing involves taking the answer to the question of your site’s ultimate purpose and turning it into something more precise: __quantifiable success metrics__. Your success metrics are the specific numbers you hope will be improved by your tests.

Part of building out your testing strategy is identifying what constitutes - and does not constitute - a "conversion" for your particular site. In online terms, __a conversion is the point at which a visitor takes the desired action on your website__. Pinpointing the specific actions you want people to take most on your site and that are most critical to your business will lead you to the tests that have an impact.

#### Macroconversions, Microconversions, and Vanity Metrics

Author and digital marketing evangelist Avinash Kaushik makes the distinction between what he calls \
- macroconversions - the metric most closely aligned with your site’s primary raison d’etre
- microconversions - the other actions that users take on your site. 

While __microconversions__ (things like clicking one button in a signup funnel, watching a video, or commenting on a blog post) may not be as immediately valuable or monetizeable as __macroconversions__, they can provide a tremen- dous amount of indirect value (provided they’re not optimized at the expense of macroconversions).

> A quick word of caution: sometimes a business can be lured into chasing “vanity metrics” that end up being distractions from the actual goal.
>
> Consider a hypothetical business-to-business (B2B) software company’s blog. The marketing team wants the blog to be a hub of thought leadership in their industry. Since they’re A/B testing the main site, they decide to start optimizing the blog, too. On the main site, their aim is clear: to use A/B testing to help drive more free trial signups. Defining quantifiable goals for the blog is harder for the team, so they have been unable to define what makes an A/B test successful.
>
> For the B2B blog, a vanity metric could be headline clicks. If this is the only piece of data you’re using to determine whether the blog is successful, you could be optimizing the wrong thing. Maybe people click headlines because they are shocking, but don’t read past them. If all you measure is clicks, you’ll never know whether the content of the actual post is good. More telling metrics might be call-to-action clicks, comments, shares, and repeat visits.

### Step Two: Identify bottlenecks

Bottlenecks are the places where your users are dropping off, or the places where you’re losing the most momentum in moving users through your desired series of actions.

### Step Three: Construct a hypothesis

Once you’ve identified what the bottlenecks are in your process, use your understanding of visitor intent to come up with test hypotheses. Consider different forms of qualitative research such as user interviews, feedback forms, or focus groups to gain an understanding of what’s going on in users’ heads as they interact with your site.

“Failed” tests are valuable because they often lead to new hypotheses for why they didn’t turn out the way you expected. Generating these hypotheses is sometimes tricky, because visitors behave in complex ways. Regardless of the complexity, however, employing the scientific method in testing will bring you closer to a meaningful understanding of your website’s audience.

### Step Four: Prioritize

Once you’ve generated hypotheses about user behavior that lead to candidate page variations for testing, you’ll need to use your intuition about what’s going to have the biggest impact to rank- order your experiments.

> “Use ROI [return on investment] to prioritize your tests,”

says Kyle Rush, who was the Deputy Director of Frontend Web Development at Obama for America. 

> “That’s one of the biggest things I’ve learned in my career.”

In a perfect world, you might test absolutely everything, but no team in the real world operates without constraints; your team’s attention, budget, time, and also your site’s traffic are all finite. These realities make prioritization of testing hypotheses a necessity.

### Step Five: Test

Once the test reaches statistical significance, you’ll have your answer.

__Often a completed test yields not only answers, but - as in any other science - more questions.__

> TL;DR
>
> - You can’t pick a winner until you decide how you’re keeping score. A/B testing starts with determining quan- tifiable success metrics.
>
> - Thereareanumberofpossibleconversiongoals:timeon site, pageviews, average order value, revenue per visitor, and so on. Take the time to pick the one that’s right for you.
>
> - Site analytics along with your own instincts will suggest bottlenecks where you can focus your attention.
>
> - Understanding visitor intent with the help of interviews and usability tests will suggest hypotheses about what to change and how.
>
> - Prioritize your experiments based on your prediction of their impact.
>
> - Start testing, and continue until you begin to see dimin- ishing returns.

---

## Chapter 3 - Seek the global maximum

While refinement can lead to a solution better than what you have today, we recommend exploring multiple alternatives that might not resemble the current site first. We encourage the kind of humility and bravery required to say, 

> “You know, the website we have today is far from perfect. Let’s try some dramatically new layouts, new designs, and redesigns, figure out which of those work well, and then refine from there.”

However, it’s not as simple as saying that one should always explore first and always refine second. The truth is that exploration and refinement are complementary techniques, and most effective when used in tandem.

---

> Break from the Status Quo: ABC Family
>
> Disney ran an experiment using Optimizely on the ABC Family homepage.
> The page displayed a large promotion for a television show you might be interested in. After looking through their search logs, however, the Disney digital team discovered that a lot of people were searching for the exact titles of shows and specific episodes. Instead of taking the incremental approach (e.g., by tweaking the promo image, or rotating the featured show), the team decided to reevaluate their entire approach. They created an alternative view, one that was less visual and more hierarchical, in which users can drill down through menus to specific shows.
>
> Disney had defined as their quantifiable success metric the percentage of visitors who clicked on any part of the experiment page. Their goal was to lift this engagement by 10 to 20 percent. In fact, by being open to this big, fundamental change, they were able to effect an engagement increase of more than 600 percent.

---

> Learn Your Way to the New Site: Chrome Industries
>
> Kyle Duford at cycling bag and apparel manufacturer Chrome Industries explains that the Chrome team is presently discussing a major site redesign. “We’re purposely using all of these tests to formulate how we approach the new website.”
>
> The Chrome team discovered something surprising when they were A/B testing the order of the three promotional content blocks on their homepage: the content they put in the center block seemed always to outperform the content they put in the left block.
> 
> The team’s assumption was that because people read from left to right, they would explore in this manner. “This is gold,” says Duford. Now they know to put their most important promo block in the center, but the bigger lesson is that users seem to go straight for the central imagery, rather than scanning left to right. 
>
> This is a valuable insight that may end up altering the entire new layout for the site redesign. “The look and feel will be completely different, but the ideas of the blocks of content that go into it are all being discovered through this process,” Duford says. “So while it’s important right now to understand how people shop, it’s more important because it’s going to inform our decisions going forward.”

---

> Rethink the Business Model: Lumosity
>
> Lumosity is a company that offers web-based games designed to improve users’ minds. Their business model is simple: users pay a monthly subscription fee to access games designed by neuroscientists to promote cognitive function. Users derive the most benefit from training regularly, and boosting user engagement was an important goal. What wasn’t intuitive, however, was what the Lumosity development team did to increase this metric.
>
> Lumosity’s scientists recommended that users train for 15 to 20 minutes a day, 4 to 5 times per week - not unlike physical exercise -although the site didn’t actually constrain users to a specific time limit. The data showed that people would stay logged in for many hours, but that over time, the frequency of logins declined, suggesting users were burning out.
>
> The team hypothesized that limiting the amount of training a user could do in one day would improve engagement over time.
>
> Giving users one training session a day and congratulating them on being done for the day might achieve their goal. Such a radical change initially made many people at the company nervous, including Product Manager Eric Dorf, who feared that restricting the amount of a time a user could use the service they were paying for would frustrate the user base. 
>
> “I felt like, if I’m paying for this as a subscription and I’m not allowed to train as much as I want, why would I pay for it?” he says. “I remember thinking, ‘Gosh, I hope we don’t piss everybody off.’”
>
> Trying out the new model as part of an A/B test mitigated that risk. The team ran an A/B test that set the original, unlimited training against the limited-training variation. The results shocked Eric and his team. Users actually trained more over time in the new model. “The graph was so clear,” Eric says. “People were training more as a result of being limited.”
>
> After making this discovery, the Lumosity team changed the way they position, build, and sell their program. The message of daily training is the cornerstone of their communications to users. After this initial exploration, the team then subsequently used A/B testing to refine the approach, finding the messages and marketing that best support and reinforce the idea of daily training.
>
> Today, when a user completes a session, the message is, “You’re done. You can leave the site now,” Dorf explains. “It’s not like a lot of other gaming products that want you to spend all your time playing. The scientists are happy because more users are more engaged with training than before.”

---

> Test Through the Redesign, Not After: Digg and Netflix
>
> ---
>
> When it comes to making better data-driven decisions, the sooner the better. Often the temptation is (and we’ve heard this before) “Oh, we’re doing a redesign; we’ll do the A/B testing afterwards.” The fact is you actually want to A/B test the redesign.
>
> Around 2010, we were introduced to the folks at Digg by their new VP of Product Keval Desai to talk about using Optimizely. Their response was, “We are busy working on a complete overhaul of our site. After we do that, then we’ll do A/B testing.”
>
> As Desai explains, the “Digg v4” redesign was a perfect storm of problems. The company rolled out a new backend and a new frontend at the same time, conflating two different sets of challenges. “It was a big bang launch,” he says. The backend couldn’t initially handle the site traffic and buckled on launch day. What’s more, despite faring well in usability tests, focus groups, surveys, and a private beta, the new frontend met with vociferous criticism when it was released to the public, and became a magnet for negative media attention. 
>
> “When you change something, people are going to have a reaction,” Desai says. “Most of the changes, I would say, were done for the right reasons, and I think that eventually the community settled down despite the initial uproar.” But, he says, “a big-bang launch in today’s era of continuous development is just a bad idea.” “To me, that’s the power of A/B testing: that you can make this big bet but reduce the risk out of it as much as possible by incrementally testing each new feature,” Desai explains. People are naturally resistant to change, so almost any major site redesign is guaranteed to get user pushback. The difference is that A/B testing the new design should reveal whether it’s actually hurting or helping the core success metrics of the site. “You can’t [always] prevent the user backlash. But you can know you did the right thing.”
>
> ---
>
> Netflix offers a similar story of a rocky redesign, but with a crucial difference: they were A/B testing the new layout, and had the numbers to stand tall against user flak. 
> 
> In June 2011, Netflix announced a new “look and feel” to the Watch Instantly web interface. “Starting today,” wrote Director of Product Manage- ment Michael Spiegelman on the company’s blog, “most members who watch instantly will see a new interface that provides more focus on the TV shows and movies streaming from Netflix.”
>
> At the time of writing, the most liked comment under the short post reads, “New Netflix interface is complete crap,” followed by a litany of similarly critical comments. The interface Netflix released to its 24 million members on that day is the same design you see today on netflix.com: personalized scrollable rows of titles that Netflix has calculated you will like best. So, in the face of some bad press on the blogosphere, why did Netflix decide to keep the new design? The answer is clear to Netflix Manager of Experimentation Bryan Gumm, who worked on that redesign: the data simply said so.
>
> The team began working on the interface redesign in Janu- ary 2011. They called the project “Density,” because the new design’s goal was literally a denser user experience.
>
> The original experience had given the user four titles in a row from which to choose, with a “play” button and star rating under each title’s thumbnail. Each title also had ample whitespace surrounding it a waste of screen real estate, in the team’s opinion.
>
> The variation presented scrollable rows with title thumbnails. The designers removed the star rating and play button from the default view, and made it a hover experience instead.
>
> They then A/B tested both variations on a small subset of new and existing members while measuring retention and engagement in both variations. The result: retention in the variation increased by 20 to 55 basis points, and engagement grew by 30 to 140 basis points.
>
> The data clearly told the designers that new and existing members preferred the variation to the original. Netflix counted it as a success and rolled the new “density” interface out to 100 percent of its users in June 2011. As Gumm asserts, “If [the results hadn’t been] positive, we wouldn’t have rolled it out.” The company measured engagement and retention again in the rollout as a gut-check. Sure enough, the results of the second test concurred with the first that users watched more movies and TV shows with the new interface.
>
> Then the comment backlash started.
>
> However, as far as Netflix is concerned, the metrics reflecting data from existing and new members tell the absolute truth. As Gumm explains, the vocal minority made up a small fraction of the user base and they voiced an opinion that went against all the data Netflix had about the experience. Gumm points out, “We were looking at the metrics and people were watching more, they liked it better, and they were more engaged in the service.
>
> ... [Both the tests] proved it.”
>
> Gumm also makes the following very important point: “What people say and what they do are rarely the same. We’re not going to tailor the product experience, just like we’re not going to have 80 million different engineering paths, just to please half a percent of the people. It’s just not worth the support required.”
>
> Gumm then reminds us that despite the few loud, unhappy customers that may emerge, the most critical thing to remember is the data: “I think it’s really important in an A/B testing organization, or any data-driven organization, to just hold true to the philosophy that the data is what matters.

> TL;DR
> - Incrementalism can lead to local maxima. Be willing to explore to find the big wins before testing smaller changes and tweaks.
> - Conversely, sometimes it’s the incremental refinements that prove or disprove your hypotheses about what your users respond to. Use the insights from small tests to guide and inform your thinking about bigger changes.
> - Consider entirely new alternative approaches to your principal business goals. Be willing to go beyond just testing “variations on a theme”—you might be surprised.
> - If you’re working on a major site redesign or overhaul, don’t wait until the new design is live to A/B test it. A/B test the redesign itself.

---

## Chapter 4 - Less Is More: Reduce Choices - When Subtraction Adds Value

Sometimes the winning variation of a page is one in which you haven’t added anything at all but in fact removed elements from the page. We have seen many teams improve conversion metrics simply by adhering to the design mantra “Less is more.” The products of this approach - simpler pages, shorter forms, and fewer choices—can make a very big difference.

### Every Field Counts: The Clinton Bush Haiti Fund

When you’re asking the user to take an action, every bit of effort counts, and so we wanted to look at the form to see if there was any way we could streamline the user’s experience. 

> We noticed that the Foundation had included fields for “phone number” and “title,” hoping down the road to be able to use this information, possibly to make phone solicitations. 
>
> The fact was, however, that the Foundation was stretched so thin that it wasn’t actually calling anyone, so this additional information being requested of users wasn’t being put to use. 
>
> We hypothesized that getting rid of these two optional fields, even if it came at the cost of some potentially useful data, would be more than made up for by added donations in virtue of the simpler form.
>
> The effect was instantly measurable and dramatic. Simply removing two optional fields resulted in an 11 percent improvement in dollars per pageview over the length of the test - a massive gain in donations from a small simplification.

### Keep It Simple: SeeClickFix

> SeeClickFix is a web tool that allows citizens to “report neighborhood issues, and see them get fixed.” The tool centers on a web-based map that displays user activity. Users add comments, suggest resolutions, and add video and picture documentation. Anyone can elect to receive email alerts based on “Watch Areas” by geographical area and can filter reports by keyword.
>
> The original SeeClickFix homepage contained a simple call to action with one form and a simple design. After a great deal of work by the team’s designers and engineers, SeeClickFix had a brand-new homepage ready to launch, complete with an interactive map. 
>
> The team was excited about it, and used an A/B test to find out just how brilliant their new design idea was.
>
> They were in for a surprise. SeeClickFix actually drove 8 percent more engagement on the simple gray box form that displayed a simple call to action and a description. The proposed new homepage may have been more technologically sophisticated and visually rich, but simplicity mattered where it counted most: getting visitors to engage with the site.

### Hide Options: Cost Plus World Market

The checkout funnel is a prime place for optimization on a site, and a place where it’s often true that less is more. 

This makes intuitive sense: nonessential steps included in the purchase process can be distracting, and it’s no surprise that minimizing obstacles frequently boosts conversion. What’s interesting is that removing unnecessary options can also reduce the overall friction of the process in a significant way.

> Cost Plus World Market, a chain of specialty/import retail stores and a subsidiary of Bed Bath & Beyond, ran an experiment that tested hiding the promotion code and shipping options form fields from the last page in the checkout funnel.
>
> By hiding these two optional fields and making them expandable links instead, Cost Plus saw a 15.6 percent increase in revenue per visitor. Conversions also went up by 5.2 percent in the variation with hidden fields.

### Remove Distractions and “Outs”: Avalanche Technology Group

It’s important to remember that any clickable element on a page represents an option for your user, not just those explicitly included in the checkout process. Our next example illustrates how valuable it can be to focus on those elements that aren’t actually part of the checkout process.

> Avalanche Technology Group is the Australian distributor for popular antivirus software AVG. When they examined their shopping cart conversion data they suspected there was room for improvement, and wanted to experiment with some “minor” (or so they thought) variations that would leave the actual steps of the checkout process untouched.
>
> The team decided to run an experiment in which the site’s header navigation links were removed from the checkout funnel, which they hypothesized would reduce visual noise and keep traffic more focused on actually checking out.
>
> This change alone improved conversion rates by 10 percent and led to a 16 percent increase in revenue per visitor, showing that even visually minor changes to the “auxiliary” parts of a page can have a big impact on visitor behavior.

### Lower the Slope: Obama 2012

> The previous examples show the potential benefits of keeping things simple by removing distractions and minimizing options. But how do you optimize when things are already as simple as they can possibly be? Or are they?
>
> Every one of the 165 people on Obama’s 2012 digital team understood how mission-critical A/B testing was to running the digital campaign. “We didn’t have to convince anybody that A/B testing was important; it was just a no-brainer,” recalls Kyle Rush, one of the lead developers on the campaign, who was responsible for much of the testing program.
>
> The Obama 2012 team executed nearly 500 A/B tests over a 20-month period. The team experimented with everything from imagery and design to copy and usability. As a result, the optimization program collectively brought in an extra $190 million in campaign donations. One of the key tests that contributed to the $190 million pot was what came to be known as “Sequential.”
>
> The original donation process was a single page with a form and a picture of the president playing basketball. This page was already highly optimized: the image had been tested and not one superfluous form field existed - only the legally required ones remained. It looked pretty, and it was converting, but the campaign team wanted to see if they could go further. 
>
> Since federal law requires specific information from campaign donors, the team couldn’t just eliminate form fields at will. On the other hand, they knew from usability tests that the form was too long and losing potential donations. What to do? The team had an idea: make the form appear shorter by breaking it into pieces.
>
> Once the form was divided into a sequence, the next logical thing to test was the order of the sequence. “Asking for the donation amount first more closely matches the users’ state of mind,” Rush explains. “Once they’ve made the decision to donate they’re ready to enter an amount, not their personal information.” Optimizations to the sequencing confirmed that donation amount should come first, then personal information, then billing, and occupation/employer last.
>
> The optimized form yielded a 5 percent conversion increase over what had initially seemed to be the maximally optimized page. As Rush puts it: “You can get more users to the top of the mountain if you show them a gradual incline instead of a steep slope.”

## Chapter 5 - Words Matter: Focus on Your Call to Action - How a Few Words Can Make a Huge Difference

__The wording on your site represents an area where there are virtually inexhaustible opportunities for experimenting with variations.__ The variations can be crafted with just a few keystrokes, and often the slightest change can have a major effect. 

Because language is so easily tweaked (compared to art or images, which require careful design work) and its possibilities are so vast, it represents a major opportunity to test variations at the speed of brainstorming itself. The words on a site are some of the most powerful and potent elements a user sees. The right combination can be leaps-and-bounds more effective than the rest.

### Reconsider “Submit”: The Clinton Bush Haiti Fund

> Instead we tried the label “Support Haiti,” hypothesizing that making the button reflect the purpose of their action would make the meaning of their clicks more immediately clear to users.
>
> The difference was enormous. The effect of the change from “Submit” to “Support Haiti” was on the order of several dollars per pageview, and this small change, together with our optimizations to the form and several other quick, simple tests, managed to bring in an additional million dollars of relief aid to Haiti. It’s a testament to the power just one or two words can have.

### Find the Perfect Appeal: Wikipedia

> For a big fundraising push, Wikipedia’s team comes up with dozens of variations to test. Finding the most effective words among the essentially endless options is a huge task—one that takes a lot of creativity and a lot testing. “You have to have a rule that if anybody feels strongly about testing something, you test it,” Exley says.
>
> There’s one test in particular that stands out in Exley’s mind. There was a fundraising appeal that had done well as part of the site’s landing pages:
>
> “If everyone reading this donated $5, we would only have to fundraise for one day a year. Please donate to keep Wikipedia free.” One of the Wikimedia team members suggested testing what would happen if they replaced the last third of their fundraising banner with this line, and Exley agreed to a test.
>
> This variation was a bit of a gambit, because setting the bar so distinctly at $5 had the potential to “anchor” users’ minds at a lower level than the one suggesting “$5, $20, $50, or whatever you can.” On the other hand, the logic of the appeal - and perhaps the very absence of higher dollar values - might persuade more users to give.
>
> The outcome: even though the five-dollar appeal lowered the average donation amount by 29 percent, the rate of donation went up by a whopping 80 percent, resulting in a net increase in overall amount raised of 28 percent.

### Why versus How: Formstack

Deciding what to include in a site’s main navigation, and how to arrange it, is key to establishing the flow of traffic and the focus of the site - and it can provoke strong differences of opinion.

> While redesigning their site, the team at online form builder Formstack sat around a table, considering their navigation. They all agreed on highlighting the form types, features, examples, and pricing, but were not sure what would be the best page to use as a lead.
>
> The team settled on using "Why Use Us" as the lead navigation item because they suspected it would help persuade visitors that Formstack is a better choice than the competition. As analytics on the new site design filtered in, they noticed that visitors weren’t clicking on “Why Use Us” as much as they had expected. 
>
> That prompted a follow-up test: they tested whether naming the page "How It Works" would prompt more visits.
>
> Although “Why Use Us?” was the question the Formstack team ultimately wanted to answer for their visitors, they decided to try the header “How It Works” because, their thinking went, it would invite visitors to investigate on their own without the obvious self-promotion.
>
> “How It Works” also helps a user unfamiliar with web form builders get his or her bearings on what it is that Formstack does as a company, whereas “Why Use Us” might suggest an explanation of how Formstack differs from its competitors, rather than what its product does in the first place.
>
> In an A/B test pitting “Why Use Us” against “How It Works,” the winner was clear. Naming the lead navigation item “How It Works” increased traffic to that page by nearly 50 percent, and also lifted two-week free-trial signups by 8 percent.
>
> “Instead of getting bogged down in disagreements, we moved forward,” says Jeff Blettner, a web designer at Formstack. “We knew that we would be able to come back after launch and test our hypotheses.”

### Nouns versus Verbs: LiveChat

> LiveChat sells software that allows businesses to talk with their website visitors in real time.
>
> In order to figure out how to maximize the company’s product sales, LiveChat visual designer Lucy Frank evaluated the steps most people take in signing up for the service.
>
> She found that most visitors sign up for a free trial before becoming paying customers, and so she hypothesized that increasing the number of people in free trials might result in more sales downstream.
>
> Since the first step to starting a free trial is clicking the big shiny button on the homepage, Frank began her experimentation there. She decided to simply change the call-to-action text on the button from “Free Trial” to “Try it free,” and see which version enticed more users to register.
>
> The team hadn’t expected to see much of a variation in terms of results from making such seemingly small changes. Yet the difference of just two words increased click-through rate by 14.6 percent.
>
> This experiment is a great example of what we’ve seen again and again across a wide range of businesses. We usually give folks some pretty straightforward advice when they ask about how to improve their calls to action: verbs over nouns. In other words, if you want somebody to do something, tell them to do it.

### Framing Effects

There are endless possibilities for any call to action, and it’s not feasible to test them all. So, how do you focus your tests on only the possible alternatives that are most likely to have an impact? Having a good hypothesis of why a change will be effective is a crucial step, and one powerful theory to help you formulate it is called __framing__.

Framing is the simple idea that different ways of presenting the same information will evoke different emotional reactions, and thus influence a person’s decision. For example, as Nobel laureate psychologist Daniel Kahneman notes in Thinking, Fast and Slow:

> The statement that “the odds of survival one month after surgery are 90%” is more reassuring than the equivalent statement that “mortality within one month of surgery is 10%.” Similarly, cold cuts described as “90% fat-free” are more attractive than when they are described as “10% fat.”

> A famous 1995 study by psychologist Sara Banks et al. involved showing two groups of women videos on breast cancer and mammography in an attempt to convince them to get screened. 
>
> The first group’s video focused on gains, that is, it espoused the benefits of having a mammogram.
>
> The second group’s video was loss-framed: it emphasized the risks of not having one. Though the two videos presented the same information, only 51.5 percent of those who saw the gain-framed video got a mammogram in the next year, whereas 61.2 percent of those who saw the loss-framed video did so.

__There are no one-size-fits-all rules about message framing__, and it’s still important to test variations. However, an awareness of framing helps to define the scope of possibilities.

You might, for instance, choose to avoid testing more equivalent phrases and consider strikingly different ways you can frame your value proposition and test each of them.

Try asking yourself:
- Is the language negative or positive? Do you, for instance, advertise what a product has or what it doesn’t have?
- Is the language loss-framed or gain-framed (e.g., the mammography study)?
- Is the language passive or action-oriented (e.g., LiveChat’s “Try it free” button)?

> TL;DR
> - There are endless word combinations to use on your website. Don’t be afraid to brainstorm and think broadly: a testing platform lowers the “barrier to entry” of ideas, minimizes the risks of failure, and enables quick and potentially huge wins.
> - Decisions around messaging and verbiage can easily lead to contentious debates on a team. A/B testing is a way to put opinions aside and get concrete feedback on what works and what doesn’t. (We’ll revisit this idea in Chapter 8.)
> - If you want someone to do something, tell them to do it.
> - Different ways of framing the same message can cause people to think of it in different ways. Play with alternative ways of framing the same information and see what differences emerge.

---

## Chapter 6 - Fail Fast and Learn - Learning to Embrace the Times When A Beats B

Even “failed” experiments have their silver linings: recognizing that a particular change will harm your goals is inarguably better than simply making that change, and as an added benefit experiments like this are often the ones that teach us the most about our visitors and what drives them.

### Prime Real Estate versus Familiar Real Estate: IGN

> Gaming website IGN wanted to encourage more visitors to the video site that brings them a big portion of their ad revenue. So they tried running an A/B test where they moved the “Videos” link over to the left of the main navigation.
>
> There are plenty of organizations in which a change like this would have come down the chain of command once-and-for-all from the HiPPO - the Highest Paid Person’s Opinion. But before making the change for good, IGN ran an experiment to see exactly how much of an increase they could expect to see from giving the “Video” link top billing.
>
> Not only did the test, shockingly, show no increase at all, it showed that the new banner dramatically reduced the video click rate by 92.3 percent. If they had blindly moved the “Videos” link without testing it first, the change could have been disastrous. Because IGN gets so much traffic, it only took them a matter of hours to get statistically significant results. They were able to cease the experiment, return to the original design, and go back to the data for more answers.
>
> The test saved IGN from a potential catastrophe that would have occurred had they simply rolled out the new navigation, but there’s a bigger lesson. One of the biggest reasons for dramatic results like this one is that a lot of a site’s traffic typically comes from returning visitors, users who are accustomed to seeing the site in a certain way - in this case, with the “Videos” link on the far-right side, not the far-left. 
>
> When it’s missing from the spot they normally go to find it, they’re not going to do the work to locate it. 
>
> Considering the root cause of the results offers lessons not only about proposed changes but, at a deeper level, about the testing process as well. Moving forward, the team can consider the fact that new and returning users are going to have very different experiences of the site. Keeping this in mind will bear fruit in subsequent tests.

### What’s Good at One Scale Isn’t Always Good at Another: E-Commerce

Many retailers have customer reviews and star ratings displayed on their sites. 

> One large online retailer discovered through testing that displaying the rating prominently on individual product pages helped conversion.
>
> So the e-commerce team there experimented with adding the ratings to the category page — the page one level up from the product page that shows all of the items in a category.
>
> It seemed like common sense to the e-commerce team: showing the stars on the spill page should motivate people to click through and view the products more often thereby increasing conversions. 
>
> Good thing they tested it because that was not the case, the variation did more harm than good. As it happened, showing star reviews made customers convert 10 percent less. The test illustrates that what works at one level of scale doesn’t always work at another level; what was good for the product page ended up being bad for the category page. 
>
> It’s a good reminder that just because something makes sense on a particular part of your site - or is even proved to be advantageous - doesn’t mean you should roll it out to other parts of the site without checking first.

### What Buyers Want Isn’t Always What Sellers Want: Etsy

> Handmade- and vintage-goods marketplace Etsy has over 42 million unique visitors per month and is among the Alexa Top 200 websites. A/B testing is an important means for the product developers and engineers to collect behavioral data about how Etsy’s 800,000 sellers and 20 million members use the site.
>
> Etsy users are shown an activity feed in which they can see highlights from fellow Etsy members they follow: the items those people are favoriting and purchasing. There are thousands of items posted to Etsy weekly, and this activity feed is a handy way for users to discover new items on the site.
>
> The activity feed displays a combination of activities happening for buyers and sellers in one list. In what the team thought would be a much improved experience, they redesigned the feed and removed the “Your Shop” view from it, leaving only the “People You Follow” view. They A/B tested the original feed against the redesigned one to see how the redesign fared with users.
>
> To the team’s surprise, engagement with the feed dramatically decreased in the variation. After a closer look at the data, they discovered a certain type of use case that the team didn’t anticipate. 
>
> It turned out that sellers were using their own activity feed to manage their shops: as a timeline of what items they had listed at what times. The team envisioned the feed as a tool for buyers to scroll through what people were doing on the site.
>
> But sellers had been using this to manage their shops and the new “reskinning” removed this functionality for them.
>
> Without this surprise result, the Etsy team would never have known about this use case. Now, not only could they take it into account during the redesign, they could actually design for it.
>
> Their next iteration included two buttons: one called “Following,” and another for “Your Shop”. The story ends happily with Etsy now actively building site functionality around a usage that, until their original redesign hit a snag, they had never known about.

### When a Win Isn’t a Win: Chrome Industries

> The Chrome e-commerce team has experimented with a plethora of image treatments for their urban biking products over the years, and recently decided to test whether a product video spurred more visitors to make purchases than did a static image.
>
> The objective of the test was to determine whether to commit more resources toward video development. The team picked one product to experiment with: their Truk shoe.
>
> They measured the percentage of visitors to the Truk product page from the category page, the percentage of visitors who continued to checkout, and percentage of visitors who successfully ordered.
>
> After letting the test run for just under three months, the results were something of a wash. Users visited the Truk product page 0.5 percent more with the image, continued to checkout 0.3 percent more with the video, and successfully ordered 0.2 percent more with the video.
>
> If anything, the video slightly edged out the static image, but because producing video involves a much higher investment from Chrome than the images, the verdict is actually a clear vote against the added production cost.
>
> Chrome can table the issue for the time being, or it can further investigate the reason why video didn’t convert, rather than moving forward under the assumption that video will drive sales and ramping up a full-blown video asset initiative that won’t necessarily prove its return on investment.
>
>If the team does choose to test video down the line (e.g., in seeing how lifestyle-oriented video might compare against product-oriented video), they can at least be confident that there’s little risk in running the follow-up test, since they’ve proven that video won’t hurt conversion.
>
> They may also decide simply to allocate their energies elsewhere and experiment with optimizing different portions of the site entirely, where there may be bigger unrealized gains awaiting.

---

> TL;DR
> - What works for returning users may not work for new
users, and vice-versa.
> - Something that works on one page may not work on another; something that works at one scale may not work at another.
> - What one type of user wants may not be what another type of user wants. A failed test, sometimes more than a successful test, may prompt a drill-down that reveals a key difference between segments of users.
> - Sometimes a variation may win against the original, but it may not win by enough of a margin to justify the implementation overhead or other drawbacks of the vari- ation that are external to the test itself.
> - Any test that reveals that an initiative isn’t performing is a blessing in disguise: it allows you to free up resources from things that aren’t working and divert them to the things that are.

---

## Choose the Solution That’s Right for Your Organization - Deciding Whether to Build, Buy, or Hire

The first step is making a high-level decision about how you’ll bring testing onboard: you can build your own testing tool in-house, buy a testing platform, or hire a consultant or agency to do the testing for you. There’s no wrong choice, and each has its pros and cons. Indeed, many organizations elect to combine more than one of these approaches. We walk you through the things to consider when making the decision about what’s best for your needs.

### Option One: Build

Building a testing solution in-house is a viable option for organizations that have significant engineering resources available. We’ve found that most companies don’t decide to suddenly build a testing tool from scratch without an engineering team that’s closely tied to the process. A homegrown testing tool is usually something that organizations add on to an already established data-gathering and analytics machine.

Building an in-house solution requires substantial engineering effort, so it’s rare for small companies with limited engineering resources to pursue this path. Typically only larger teams with specialized needs and enough dedicated engineering resources to pull it off will build a solution for themselves. 

> For example, Amazon has invested considerable effort over many years to build an extensive testing platform that is integrated closely with their website, product catalogue, and customer database.

There are many reasons why a company might choose to invest in a homegrown testing tool, but the biggest is probably the desire to run experiments that require a deep connection with proprietary internal systems, like Amazon’s customer database in the example above. 

Custom-built testing platforms can provide specialized experiment targeting capabilities, tight integration with your build/deploy systems, and the ability to experiment with complex, server-side logic, like a search ranking algorithm.

Sometimes the decision to build an A/B testing capability to the website arises out of iterative additions to an in-house analytics platform. 

> This was the case for Etsy, an e-commerce site that sells handmade and vintage goods. Today, their team runs every change or new feature release through an A/B test, but they didn’t always work that way.
> 
> Dan McKinley started at Etsy in 2007, and discusses how they didn’t do any measuring or A/B testing in the early days. “Nor did I really have any conception that we should have been doing it,” he confesses. By 2011, McKinley had noticed a pattern in the way they developed products. 
>
> As he describes it, the engineers would spend a great deal of time and effort working on a new feature up front. They would release that feature, and then talk about it at a company-wide meeting where there would be a lot of applause for the new feature. Then they would move on. 
>
> Two years later, they would eliminate the feature they had spent all their time and effort developing—because it turned out that nobody was using it. “I realized that we were failing in what we were trying to do; we just weren’t very good at realizing that we were failing,” McKinley told us. “[We wanted] to be better at realizing we were failing, and if at all possible, not fail. And that was the motivating factor in my getting into experimentation on the web.”
>
> Etsy has had access to a lot of data, data engineer Steve Mardenfeld explains, pretty much since the site launched. In turn, they have tools to collect, examine, and analyze that data in an effort to improve the user experience on the site. For example, examining the data lets them improve their search algorithms and recommendations. Whenever the engineers created a new feature, they would release it to a small percentage of users and look for any operational concerns. Once they knew it was functioning well (i.e., it wouldn’t break the site) they would release it to 100 percent of Etsy users.
>
> “So we were already doing the basic idea of A/B testing; we just weren’t actually measuring anything,” Mardenfeld says. “It just seemed like a really good fit to try to shoehorn this into the process that we already had.”
>
> Etsy’s team of 100þ engineers decided to go with an in-house testing tool because they already had the infrastructure in place to support A/B testing. All they had to do differently was start tracking how the new experience performed against the original experience.

#### The A/A Test

When building your own A/B testing tool from scratch, one of the obvious areas of concern will be simply making sure that the tool is functioning accurately.

One of the handiest ways to verify the accuracy of a testing tool is an A/A test. An A/A test, as the name implies, involves testing two identical versions of your page to ensure there are no statistically significant differences reported between them. If there are, then something fishy or erroneous may be happening with the way the test is being run or the way the data is collected or analyzed. An A/A test is a good way to assure yourself (and your boss) that your testing platform is functioning correctly.

### Option two: Buy

Let’s talk about what you get when you buy a testing platform. Most follow the Software-as-a-Service (SaaS) model; in other words, you won’t download anything or purchase a physical product. Rather, integration happens as easily as a one-time copy-and-paste onto your site, after which you access the software through the web and your tests and data live in the cloud. Buying a testing platform makes sense for a range of group sizes - individuals, small companies, and large companies.

At Optimizely we work with companies ranging from self-service startups to Fortune 100s that are equipped with large testing teams. SaaS solutions offer a number of advantages:

- __Built-in features__: An obvious advantage of buying a testing solution is that advanced testing features are included in your purchase. (You can, for example, target visitors from Facebook who see one variation, and compare them to Pinterest visitors who see a different one).
Commercial testing software is typically purchased as a subscription and many platforms offer multiple subscription tiers, with additional built-in features available in higher tiers.

- __Automatic updates__: When building a homegrown testing platform, the ability to have total control over the platform requires an ongoing engineering commitment: everything the company wants the testing suite to incorporate requires engineers to build, test, and maintain it over time.
A company using an off-the-shelf SaaS product will effectively remain at the cutting edge without additional effort.

- __WYSIWYG editing__: Leading A/B testing SaaS platforms enable marketers, advertisers, designers, and other non- technical users to easily create and test page variations, using a visual what-you-see-is-what-you-get (WYSIWYG) editor that doesn’t require writing any custom code.

- __Trustworthy reporting__: Accurate and reliable statistical reporting and calibration are essential for any data-driven organization. When you purchase an off-the-shelf testing solution, you’re buying something teams have spent time building, optimizing, and debugging. You can therefore trust them to give you accurate results about how your tests performed. What’s more, these platforms are constantly being tested by the thousands of clients using them. (If you want to test the tool’s accuracy yourself, you can of course run an “A/A test” as discussed above.)

- __Professional Support__: Most A/B testing platforms offer some form of dedicated technical support, and some even offer additional consulting services (for an additional charge). Technical support is especially important for teams in which non-technical users are driving the testing process.

- __Community__: When you sign up for an A/B testing platform, you are joining an existing community of users who are available to answer questions, give technical support, and suggest best practices.

---

Questions to Consider When Evaluating an A/B Testing SaaS Solution:

1. __Does the platform integrate with other tools you already use?__
    
    For most businesses, an A/B testing tool will complement the other tools you’re already using, especially analytics.
  
    The more your efficiency, data collection, and lead-generation/nurture tools communicate with each other, the more effectively you can use them in concert. Plus, integrating all of these can maximize the ROI you get out of each service alone.
2. __Does it meet your budget?__

    Testing solutions charge in a variety of different ways and while we encourage the reader to explore her options, ultimately the decision should come down to ROI: will the gains achieved by regular testing outweigh the usage fees paid for the platform? 
    
    If budget is a concern, it may be possible to start small and expand: for example, for companies with multiple web properties or an international presence it might make sense to start with a single property/region and expand as you begin to realize gains.

3. __How well do you gel with the platform provider’s team and support approach?__

    Because many tools can provide similar features technologically, the brand’s personality, dedication to customer success, and availability are other critical elements to consider. Support on the platform must come from the platform provider, so ensuring the help is there if you need it is important. 
    
    It may indeed be the variable that helps make your decision, since what differentiates one testing tool from the next is, in large part, the people. Are you looking for a team of people that can come up with great testing ideas for you? If you’re new to A/B testing, you might need a company that has dedicated support resources available 24/7 should you have questions.

### Option Three: Hire

The final option is to hire an agency or an optimization consultant to do testing for you.

An agency is a service independent from the client that provides an outside point of view on how best to sell the client’s products or services—or in this case, optimize the client’s website.

Most digital marketing agencies are quickly adding A/B testing to the list of services they offer; they’re also partnering with testing platforms in order to use them on clients’ websites. Companies can hire agencies or consultants as short- or long-term solutions for testing.

There are myriad reasons to outsource testing. For instance, a company that doesn’t have the internal resources to allocate to testing will instead choose to hire another entity to take care of all strategy and testing implementation. 

In another scenario, a company might have the bandwidth for ideating tests but lack the technical know-how to execute them. In this case, they’d work with an agency to implement tests. The reverse is also common, that is, for a company to purchase a testing platform and work with a strategic consultant to come up with test ideas.

If you outsource any part of testing — either the creative or the actual execution — then there are a few things to look for before you sign a contract. 

You want to make sure that the third party has a good track record with optimization strategy and implementation. They should be experts in each testing platform they offer, and provide technical support should you need it. 

With many agencies you’ll pay the agency per-hour or per-experiment, and some also offer “unlimited” or “constant” testing programs: for a fixed price they conduct an unlimited number of tests within an overall program. Compare plans and think about what will work best for your needs before signing on.

It comes down to a tradeoff between investing time and training in building a team internally, or investing money in an agency. If you’re not ready to build an internal testing team who will use a homegrown platform or an SaaS, then your best option is to hire an outside service to handle your testing.

---

> TL;DR
> - Building your own testing platform requires a significant and ongoing engineering investment, but can ultimately provide the greatest level of control and the tightest integration with your team and deployment cycles.
> - An A/A test is a helpful way to ensure that your solution is functioning, reporting, and analyzing correctly.
> - Many A/B testing Software-as-a-Service (SaaS) platforms are easy to use without requiring engineering support: marketers and product people without a coding background can create and run variations in a visual WYSIWYG environment.
> - An agency can help your team with the ideation of tests, execution of tests, or both.
> - When picking the solution that best fits your company, consider making key stakeholders part of the exploration process. The earlier you bring others on board, the easier it will be to get buy-in.

---

## Chapter 8 - The Cure for the HiPPO Syndrome - Getting Buy-In and Proving Value

### The HiPPO Syndrome Has a Cure

> DAN: I learned an important lesson about experimentation during my first year working at Google, when my mentor explained how I could convince my boss’s boss to let me try something. Even at a fairly data-driven company like Google it was hard to convince the higher-ups to do something potentially risky or radical, and it’s always a challenge to get people’s support to launch a new product or create a new feature since there’s always the concern that perhaps nobody will use it.
>
> The key phrase in receiving this higher-up’s blessing, my mentor explained, was to say, “Let’s just run an experiment.” In that context, the idea became an irresistible investigation into whether it would be worth their time and money—not a hard-and-fast decision of whether to make a permanent change.

A/B testing neutralizes the ideological and replaces it with the empirical. 

Instead of, “I feel we should do X because that’s what I feel,” a culture of A/B testing encourages both curiosity and humility, where people say, “I hypothesize that X is better than what we have right now. I don’t have the data today, but let’s run an experiment and test it.” 

After that process is complete, A/B testing then offers the much stronger claim: “I know we should do X because of the evidence that it’s the right thing to do.”

Many features and functions take longer to prioritize than they do to build. In the time you’ve fully debated whether to develop the feature, you could have already had someone implement it and gather data from an A/B test on how it works.

### Winning Over the Stakeholders

It’s also important, particularly in large organizations, to understand which key stakeholders you need to win over throughout the organization.

To do this, identify some experiments you can run that will give you early wins and allow you to make the case for a continued investment. You don’t want the first test you run to be contentious with the people whose buy-in you’re trying to win. It’s critical that this initial test clearly prove value without pissing people off.

> “What I really like to focus on are good tests, tests that you know are almost like slam-dunks or very quick wins,” Scott Zakrajsek, Global Web Analytics Manager for Adidas, advises. “What that does is help you understand how the platform works and how to use it. You can identify it as a win, pat yourself on the back, communicate that to the organization, thank everybody involved, and get everybody on board with a success.”
>
> When starting out, it’s just as important to know what not to do as what to do. One thing to avoid at the outset: running a really complex test that involves deep technical integrations. If it doesn’t work, the stakeholders may lose patience with the testing platform, or with testing itself.
>
> Whereas the homepage is frequently (and naturally) the first area of the site that comes to people’s minds for optimization, this thought is worth reconsidering, especially if your organization holds the homepage sacred. It’s typically a highly visible (thus highly scrutinized) part of the site; what’s more, the homepage is typically far from the ultimate goal for which you’re optimizing. We often recommend starting with a product page, for two reasons: it’s less visible, and it’s closer to checkout. Take some time to establish a clear case study here to prove to the folks who control the homepage that it’s worth investing and continuing to expand.
>
> “There used to be some kind of ‘sacred cows,’ that you can’t mess with this, you can’t change this,” Jarred Colli explains. After Rocket Lawyer saw an approximately 50 percent improvement in the conversion rate over a year’s worth of tests, that attitude changed completely: “Now,” says Colli, “we’ll try everything.”

> Leading human capital solutions provider CareerBuilder decided to give A/B testing a grand entrance within the company. 
>
> “When we came onboard, we actually brought several of the product owners in and had a daylong competition,” explains Senior Internal Business Systems Analyst David Harris. They paired each product with a developer and had them spend a day coming up with ideas for tests to run on their respective parts of the site. It wasn’t simply an exercise: the tests each team brain- stormed went live within the day, and then the group reconvened the following day to see which teams saw overnight results and what those results looked like. It was important, Harris says, to impart not only a sense of familiarity but also a sense of owner- ship: “This isn’t just a training. We are going to have you spend the rest of the day going in and applying this to your business, to your area of the business.”

### Communicate Findings and Value to Your Team

Communicating A/B testing’s findings and value to the team, whether it be large or small, is an important part of month one— and every month. Consider weekly, monthly, or quarterly results-sharing meetings with key stakeholders. Let them know what you’ve been up to. It will help the organization, as well as your career, because you’ve quantified your value in a way that may be difficult for roles that don’t use testing.

> “Stakeholder support and buy-in only happens if you do a good job of communicating and sharing things that you are learning,” explains Nazli Yuzak, Senior Digital Optimization Consultant at Dell. “Making sure that we are communicating our wins, communicating the learning, and sharing those large-level trends with the rest of the organization actually becomes an important part of the culture, because that’s where we are able to showcase the value we bring to the organization.”
>
> You want to let others in on what you’ve learned from your first tests. You can’t always predict who within the organization will turn out to be an evangelist for testing. We’ve seen companies handle this communication many different ways. Having built a testing culture at three large e-commerce sites — Staples, Victoria’s Secret, and Adidas — Scott Zakrajsek suggests sending straightforward emails with subject lines like “A/B Test Completion,” or “A/B Test Results.” Include screenshots of the variations and results in those emails: images are likely to be more memorable than just the results alone, as they give a clear indication of the evolution of the site over its optimization—“where we were” versus “where we are now.”

### Evangelize and Galvanize: Lizzie Allen at IGN

It’s one thing to get people excited about A/B testing; it’s another thing entirely to encourage people to make it an integral part of their daily practice. This is what it takes, though: making the testing culture at your organization contagious.

> This might seem like a daunting task. But if an entry-level data analyst can single-handedly turn a 15-year-old, 300-person company into a shining example of A/B testing prowess, you can do it, too.
>
> When Lizzie Allen joined the gaming news site IGN as a data analyst in 2010, the company had never heard of A/B testing. Allen was astounded that such a prominent content website did not use this approach to test its assumptions, especially when making editorial decisions. So she took on the challenge of introducing the company to A/B testing, and helping to establish a culture where decisions would be rooted in data. “When you tweak any sort of process in a large organization, you have to throw your weight around. When you’re entry-level, you don’t have a lot of weight to throw around. I had to use some untraditional and somewhat ballsy methods in order to promote A/B testing,” Allen says.
>
> In month one, she introduced the company to testing through training sessions. She worked with folks on the editorial team to educate employees throughout IGN about the practice. These sessions specifically centered on the value of testing two different headlines to see which one garnered more clicks. When the early buzz and excitement started to dwindle, Allen perceived that people saw A/B testing as an extra step in an already established, already functioning process. 
>
> “There wasn’t a foundation of data-minded people to support it, and I needed to cultivate that,” she says. Her strategy, then, was to shock people — a lot of people — to prove why A/B testing was so vital to business.
>
> Allen gamified A/B testing by turning the site’s tests into a competition. (IGN is a gaming site, after all.) The “A/B Master Cup” was born. Inspired by the website whichtestwon.com, Allen would once a week send out a test that IGN had run that week and ask people to choose which variation they thought had won. She used the company’s internal chat tool (Yammer) to send out screenshots of the different variations. At the end of each month, she would crown the person who picked the most test winners correctly as the “A/B Master.”
>
> She found that, overwhelmingly, everybody failed. In fact, some months went without a winner at all, because people guessed wrong so often. The contest started a lot of conversations, and began to instill a sense of humility, and also of intrigue. One week at a time, Allen built a groundswell of data-driven thinkers who were curious and eager to “figure out the puzzle that is the Internet user base for a video game content publishing site.”
>
> When asked for advice for building a testing culture at a company, Allen puts it simply:
>     
>     Be obnoxious. Question assumptions. Be that jerk in the back of the meeting who raises her hand and asks, “All right, so why are we doing this?” when everybody is going, “Fine.” That sometimes stops people in their tracks. You do that enough, and [people] will actually prepare. [They’ll start to] think about gathering metrics and data.
>
> “You are probably going to be a little bit annoying,” Allen says. But that’s okay, because even though others might not be aware of it yet, you’re changing things for the better. “Now, I walk through the halls and hear ‘Oh, we should test this, we should test that,’ she explains. “It’s almost like I need a long couch in my area so people can sit back and talk about their hypotheses. That’s the beautiful place that we’re in.”

---

> TL;DR
> - In some workplaces, and to varying degrees, the HiPPO Syndrome holds sway, but often all it takes are a few humble tests and a curiosity and interest in testing begin to take root.
> - Allay the wary with the reassurance that you’re not pro- posing sudden changes: it’s “just an experiment.”
> - In month one, run a test that is politically palatable and easy enough that it can show a quick win. Product pages can be a great place to start.
> - Share wins not only with your immediate boss but, if possible, with the company at large. Communication is key in getting people to care about testing.
> - Get creative with how you introduce stakeholders and co-workers to A/B testing.

---

## Chapter 9 - The A/B Testing Dream Team - Bringing Everyone Together

A/B testing is by its nature collaborative and interdisciplinary, straddling traditional departmental lines: marketing, prod- uct management, engineering, and design. As a result, the adoption and long-term success of testing requires thinking about how it will fit into your company, not just philosophically and culturally, but organizationally.

### The Centralized Team

> At every place Adidas’s Scott Zakrajsek has worked, the testing team has started as a team of one or two.
>
>     At Staples I was the only person and we basically had to hire up from there, so we hired a front-end web-developer who was just for testing. We hired a creative designer who was just for testing; we hired an additional two analysts.
>
>     I think your key players are someone with a good project management background to manage the pipeline. Someone with attention to detail to be the coordinator and make sure you have all the right creative assets. You need a web-analytics person to read the results and do post-test segmentation. That said, smaller companies can do it well with teams of one person. The “beg-borrow-steal” method works well, too.
>
> Zakrajsek currently runs the global analytics and optimization team for Adidas. His team handles all A/B testing responsibilities companywide, including executing tests and delivering results; if anyone has an idea for a test, they tell Zakrajsek, and his team knows what to do with it.
>
> The strategy over the longer term, however, is to make this team the centralized keeper of institutional testing knowledge while eventually decentralizing the test execution: entrusting product managers with responsibility of running tests for different areas of the site.

### The Decentralized Team

The alternative to a centralized structure, where there is a single testing team to whom all other departments come for testing ideas and/or execution, is a __decentralized__ structure, where each product manager (PM) has autonomy, the responsibility for a different part of the website, and the authority for testing that set of pages.

> CareerBuilder is another company that has moved in this direction. 
>
>     “Previously most of our testing had resided within our development area,”
> explains Senior Internal Business Systems Analyst David Harris. 
>
>     “If one of the product owners were looking to experiment with something, they had to submit something down into our development group, who would make those changes and test them on their own, so [testing] was kind of siloed out by itself a little.”
>
> The adoption of a no-code-necessary A/B testing platform enabled a change.
>
>     “Part of what we were eager to do was really to move that into the hands of people who had a more direct stake in that particular page or area, to be able to make those changes themselves and then bring a data-supported request to development as opposed to just submitting the test and waiting for the result.”
>
> Harris is the point person for all things A/B testing from an administrative standpoint: he trains every new tester on how to use the tool and best practices, and if people have quality assurance questions (like how long to run a test), he takes care of it. However, he is not part of the process whereby product owners decide what to test. And he explains why: 
>
>     “We want to give people room to have creativity and freedom to do what they’re doing within those [product] areas.”
>
> A decentralized model allows for greater independence, and reduces the potential for an organizational bottleneck, as each team can be testing its own part of the site in parallel. However, the disadvantage in comparison to a centralized testing team is that these separate testers face the challenge of staying coordinated and in communication about their results and best practices.
>
> At Netflix, A/B testing also happens at the product manager level: each PM operates testing and analytics for a specific streaming platform such as Xbox, web, or tablet.
>
> Bryan Gumm is Manager of Experimentation for the PS3 and Wii consoles, and has worked on various other platforms in the past. 
>
>     “We change that up about every three to six months just so all of the analysts are well-versed in all areas of the product,” he says.
>
> Twice a week, the vice presidents of product and product managers meet for strategy meetings to review and analyze test results and vet ideas for future tests. 
>
>     “Every six to eight weeks, there’s a Customer Science Meeting with our CEO and basically all of the C-levels and we present what we’re testing, what has been rolled out, and what is testing and is not being rolled out because it didn’t work,” Gumm explains.
>
> CareerBuilder has also focused on how to balance the autonomy of their different testing groups with the need for coordination among them, and part of their solution has involved the creation of internal distribution lists around testing. 
>
>     “We basically encourage anyone who is conducting a test to push out a communication to everyone that’s within that distribution before they set it, or at the time they set it live,” Harris explains.
>
> In that communication, the tester includes screenshots of the original page and variations being tested and a short write-up of the test goal. When the test has reached a statistically significant conclusion, the tester sends a follow-up with results and the key takeaways. Communicating findings and best practices is especially important for the internal team when the test requires code to set up.
>
> With testing teams in 47 different countries, Dell takes a similar approach to transparency into what’s being tested and what’s coming up. 
>
>     “You’ve got to make sure you have a process to bring everybody together,” explains Dell Marketing Director Ed Wu. “We have a meeting every two weeks with all the global and regional stakeholders and we say, ‘Guys, here’s the goal we have over the next cycle, here’s the test idea we have,’ and they can all lock in on the same test idea.”

### The Three Key Ingredients of a Scalable Testing Strategy

At Optimizely we’ve worked with companies of every size who are establishing testing teams, and over the years we’ve noticed that the most successful teams included four key elements:

1. __A point person__

    What we have heard again and again from the companies we’ve worked with is that no matter how large or small the team of people executing A/B tests is, at least one person has to live, breathe, and evangelize optimization. The first step is to give someone ownership of testing. This individual might have other jobs besides A/B testing: Lizzie Allen, for instance, held this role alongside her other data-analyst duties at IGN.

    If no one at your company is willing to take on the responsibilities, we encourage you to invest in hiring someone who can dedicate themselves to A/B testing. While you know by this point that A/B testing is tremendously valuable, it’s easy to neglect it in favor of more “urgent” tasks. There’s usually a blog post, a product release, or a code review that can take precedence. In reality, in terms of the bang for the buck, A/B testing is one of the best possible investments of an organization’s resources because it makes every dollar spent on other marketing activities more effective. Without clear direction on who will own A/B testing, however, you risk its becoming another item on the to-do list. To ensure that it gets done, make it part of a job description.
2. __Advocates across your organization__

    Your A/B testing point person (or team) won’t be effective in isolation. They’ll need allies across your marketing department and your engineering and product teams to be successful. This is not only because of the help they can provide in generating hypotheses and implementing experiments, but also because they’re key in ensuring that testing is a regular part of the product/marketing/design planning process.

    As Wu explains, allies are pivotal to growing a testing culture:

        At Dell we have global stakeholders, the teams responsible for longer testing programs, and we also have regional stakeholders that are responsible for day-to-day, weekly, monthly revenue generation, lead generation, etc. Most of [the regional stakeholders] have become believers of A/B testing and analytics. They have become champions for us. The testing teams ourselves, we are basically the carpenters - we founded our program and over time [the regional stakeholders] have become strong believers of ours so we can continue to fund the growth of our program.
3. __Enabled testers__

    Empowering and enabling the people in charge of running the tests is one of the most important components of building a testing culture. The most successful customers we work with are the teams who allow people to test with creativity and resources.

    What do we mean by this? Testing cannot work without creativity, that is, thinking outside of what exists on your site today and having the willingness to test it. Successful testing also requires certain resources: training, design/engineering work, educational tools, best practice information, and insti- tutional encouragement. It’s vital to create an environment where the people doing testing feel enabled and have the resources they need for the team to reach its full potential.
4. __A track record__

    As the volume and impact of your tests increases, so does the need to keep track of what you’re testing. First, come up with a naming convention for your tests. We recommend using initials when naming tests: a lone tester today may be part of a bigger team in a year, and so it’s important to know who ran the test. You need to establish accountability and transparency early on.

    Once you have a naming convention for your tests, start a log of tests run with their results: what worked, what didn’t work, and why. As your company (or testing team) grows, it becomes crucial to educate new people on the past lessons. It’s also extremely wise to have a record that will persist even if the tester leaves the company or role. It’s dangerous to trust one person within an entire organization to keep this testing history in his or her head: if that person leaves, you risk losing all of those pieces of wisdom. Keep a shared document or internal wiki of your tests and include screenshots. It will be a treasure trove to you.

    Finally, if you run a test that requires a particularly complex or nuanced technical integration, use the log to record how you did it. Teams down the road may want to run a similar test, and you’ll save them from having to reinvent the wheel.

---

> TL;DR
>
> - A/B testing is by nature interdisciplinary and cross-
departmental. Collaboration is key.
> - Some companies have a centralized testing team res- ponsible for coordinating and executing all A/B tests companywide.
> - Other organizations adopt a decentralized model, where each product owner/manager is responsible for testing within his or her product domain.
> - Regardless of which structure you adopt, make sure there is at least one point person within the organization whom people can come to about all things testing.
> - Ensure your point person or team maintains allies across your organization to ensure that testing is part of your planning process.
> - Make sure that your testers are enabled and empowered to be creative, and that the friction of running and iterating new tests is low.
> - Maintain records about who tested what and when, how the test was set up, and what the result was. This will enable your organization to work collaboratively to build up a set of collective wisdom and best practices.

---

## Chapter 10 - Iterate, Iterate, Iterate - The Art of Asking Many Small Questions Rather than One Big One

When test results first start to come in, it’s important to recognize that they’re likely going to generate more questions than answers. These questions will likely also point to more assumptions you find you want to test. That’s okay; in fact, that’s how it’s supposed to work. And that’s the perfect starting point for your tests to come.

One of the biggest questions that we had when starting Optimizely was whether people would continue experimenting after they found a “local maximum,” that is, the best set of tweaks for their current design or current funnel. We worried that people would spend a few months optimizing a site — a better headline here, different image there — and be done with it. We didn’t want businesses to think of A/B testing as a finite, onetime process, and we acknowledged the disconcerting possibility that businesses would feel satisfied if the site worked better than before, and might throw in the towel and declare, 
> “Well, enough of that! We’re all done A/B testing.”


Much to our delight, we’ve found that the opposite has occurred. After a few big wins, most people realize that they’re just beginning the testing journey — and the success they’ve enjoyed propels them to keep going.

### Multivariate Testing and Iterative Testing

It’s possible to create large-scale tests to assess a number of different variables simultaneously, and these big, compound experiments are known as __multivariate tests__. 

For instance, two different button colors, three different calls to action, and five different images could all be tested at once, making for a total of 2 x 3 x 5 = 30 different page combinations being shown to different users!

You may recall the example that began this book, from the Obama 2008 campaign: the campaign team used a multivariate test to optimize the best button and best media simultaneously, testing every combination of buttons and media.
One of the questions that we hear frequently is, “When should we use multivariate tests, and when should we use a sequence of single-variable tests?” It turns out this is actually a very nuanced question.

Among the most important things that multivariate tests enable you to discover are interaction effects between the different variables or elements you’re testing. It might be possible, for instance, that image X performs worse than your control, and so does button Y — yet when X and Y are shown together they are significantly superior to the control.

It’s a fear of these interaction effects that causes many people initially to assume that everything they want to test should be run as one massive multivariate experiment. We think that these fears are often misguided. Here’s why.

For one thing, interaction effects do exist, but in practice they’re relatively rare. Note that the button that performed best in the Obama 2008 multivariate test was also the button that performed best overall without taking the media into account. 

Likewise, the media that performed the best in the multivariate experiment was also the media that performed best overall without taking the button into account. The big multivariate test confirmed that these two indeed worked well together: there weren’t any interaction effects that might have complicated those single-variable results.

Even if interaction effects are rare, why not just use multivariate tests for everything, just to be sure? The fact is that multivariate tests require much more traffic to produce statistically significant results for each combination. By multiplying the number of permutations, you’re also multiplying the number of users that will need to go through your experiment, and the time you’ll need to let the experiment run before you get your answer.

Having worked with thousands of customers in a wide variety of industries, we’ve found that the people who use website optimization most effectively run a series of simple A/B tests, and then incorporate the winner as they go along. They run four or five variations, figure out what works, incorporate that element, and then move on to the next test. When you judge the risk of interaction effects to be low, we strongly recommend that you test nimbly: start with simple independent A/B tests and iterate instead of trying to sort out everything all at once in a multivariate experiment.

> “[Testing] gives us humility,” says Principal Engineer Dan McKinley at Etsy. “It changes the way we build things.” He explains, “There has been a dramatic change in the way we try to build products between 2007 and now, in that now . . . we are going to get there through smaller releases of things that we measure. We are not going to try to do 11 things at once. We’ll if possible do 11 things in sequence.”

### There Are No Universal Truths: Always Be Testing

The world is a very big place; different websites and different products appeal to different people. One of the reasons why A/B testing is so important is that t__here are no universal truths when it comes to design and user experience__. 

If universal truths existed, then A/B testing wouldn’t: you’d just look at the rulebook. But because no two audiences are the same—and people are coming from an array of places and perspectives — it’s crucial to understand and optimize the experimentation process.

More and more businesses have done so. It’s typical to see companies running a number of tests at any time, relative to the time of year, product launches or various campaigns.
What does a long-term A/B testing strategy look like? As Senior Product Marketing Manager Jarred Colli, who led A/B testing for Rocket Lawyer, puts it: 
> “Shifting the discussion from ‘What’s testable?’ to ‘Everything is testable.’”

Adopting the mantra “Always Be Testing” is one of the tenets of taking your testing program long-term.

### Redesigning Your Redesign: CareerBuilder and the Optimizely Website

One of the mistakes we have seen companies make is undertaking a complete redesign of their site and then optimizing the new site using A/B testing. This is a violation of two core principles of A/B testing: define success metrics and explore before you refine.

The failure to define success metrics comes as a result of redesigning the site without a goal. Other than wanting to look more current, what are the specific targetable behaviors or actions you want the redesigned site to encourage in your users?

The failure to explore before they refine comes when companies pass up their biggest opportunity to get hugely meaningful data from their redesign: testing the new design itself. 

__Instead of comparing the new page against a tweaked version of itself, test it against the old site.__ The bigger the change you’re making, the more you want to be sure that it’s having a positive effect. Then you can worry about refining from there, once you’ve rolled out the new design with evidence that it’s doing a better job than the old one in the critical areas.

David Harris, Senior Internal Business Systems Analyst at CareerBuilder, talks about how the long-term adoption of A/B testing at CareerBuilder has involved what he calls “pulling testing ever farther up in the process”: moving it from deployment to design.

> “It is during the very early stages of redesigning a page that we are incorporating in plans to test things — how we want to test them, what we want to test for, what conversions are important.”

> During the summer of 2012, we at Optimizely were planning a total website redesign. We felt there was an opportunity for us to hone our look and our message for new visitors. What better way to improve our own website than to eat our own dog food? We ran Optimizely on Optimizely.
>
> Our previous site design made a clear and simple statement about our initial offering, an easy-to-use product for website A/B testing which we captured with our tagline: “A/B Testing You’ll Actually Use.” We discussed the opportunity we had to reimagine a design that would maintain our core message and brand while providing additional benefits to visitors to our homepage and more broadly capture the value of A/B testing.
>
> One other area of focus with the new site was providing a more comprehensive approach to engaging different visitor types. We have continued to learn over time how different customer types use Optimizely to achieve a wide range of different goals. To address this, we built out a series of pages that focused on the benefits of using Optimizely for each of these groups: agencies, developers, e-commerce sites, large “enterprise” sites, publishers, and small businesses.
>
> Applying the approach of refine, explore, refine, our design team took to developing several concepts that could possibly achieve a new look and feel that could take our site experience to the next level in several areas. Ultimately, we agreed that the most important thing a potential tester can do on our site is enter a URL and experience our WYSIWYG editor firsthand. We focused our design efforts and our testing success metrics around maximizing the number of users who used our editor, and who signed up for an account. In addition to the key macro-conversion goals, it’s important to track a wide range of goals to get a holistic sense of how the new site design performs.
>
> In just about every metric we measured, the new site was a clear winner against the old one. After running the test on new visitors for just over a month, we were confidently able to declare a winner and push the new site live to all visitors. Most of our thoughts about what our original site lacked were proven true.

---

> TL;DR
> - Multivariate tests are a powerful way to test a number of variables simultaneously, and can reveal interaction effects between them. However, they require more traffic and can be slower to achieve statistical significance.
> - We have found that the companies that have had the greatest success with A/B testing favor a nimbler, more iterative approach that tests a handful of different var- iants of a single variable at a time and incorporates the winner as they go on to the next test.
> - When working on a complex change like a site redesign, we recommend that you move testing further up the process so that it becomes something that happens during the design and rollout of the new site, rather than after.

---

## Chapter 11 - How A/B Tests Can Go Awry - Potential Mistakes and Pitfalls to Avoid

### Testing without Traffic

The good news is that you need only two things to conduct an A/B test:

- a website with some content on it, and 
- visitors. 

The more traffic you have the faster you will see statistically significant results about how each variation performed.

What A/B testing your site can’t do, however, is generate that traffic in the first place. A blogger who’s just getting off the ground and has only 100 visitors per month would be better off focusing primarily on content and building a following of users (bolstered perhaps by SEO or paid ads) who provide traffic to the site before delving into the statistics of optimizing that traffic. 

After all, you have to generate the traffic in the first place before you do anything with it. (In addition, in a site’s fledgling period, a handful of conversations with real users will offer more feedback than you will get from an A/B test on a sparsely trafficked site.) 

While optimization can help even the smallest enterprise, it’s also true that testing becomes faster, more precise, and more profitable the more user traffic you have to work with.

### The Beginning of the Funnel versus the End: UserVoice

Testing will occasionally reveal a change that increases one metric but decreases another.

> The testing team at online help-desk software provider UserVoice hypothesized that removing fields from their free- trial signup form would increase the trial signup rate.
>
> After the UserVoice team A/B tested the page, they found that, indeed, the fewer fields they required, the more people signed up for a free trial.
>
> However, they began to feel that perhaps they’d made it too easy to run a trial. A plethora of people had begun trials, but the sales team was finding it increasingly difficult to determine who the best leads were.
>
> The first thing they needed to do was decide what kind of leads they were going after. Were they seeking individuals who were well educated and informed about what UserVoice was and what it could offer them? Or did UserVoice plan to educate these people via email marketing after they became leads? Both scenarios are plausible and reasonable.
>
> Here’s where the key performance indicators (KPIs) began to clash. The marketing team was motivated to reach the broadest audience possible, while the sales team was motivated to turn the highest percentage of the most promising leads into customers. With the question of the free trial, it wasn’t immediately clear which goal was best served in which way.
>
> The UserVoice team began to notice that homing in on immediate metrics like the percentage of users that went from one page of their signup funnel to the next made it easy to get the wrong result from their A/B tests. 
>
>     “It’s what I call pushing failure down the funnel,” says co-founder and CEO Richard White. “We want quality leads, not quantity leads,” he explains.
>
>     Getting somebody to a page and getting somebody to use a product day-in and day-out are two different things. You end up with this insidious kind of race to the bottom, where you just want to remove all of the fields and everything and just say, “Go get your trial,” and then they get into the trial and they have no idea why they’re there.
>
>     The goal will be a marketing site which should be educating them to get to the next step, so I think the tricky thing about A/B testing is the right variation may be the one where 80 percent of the people drop out on one page because they’re thinking this is not for them, but the other 20 percent of the people love it and sign up and try it.

### Off-Brand Testing: Dell


There’s a joke among A/B testing veterans that almost any variation of a button loses to a button that says, “Free Beer.”

> Eric Ries, author of The Lean Startup, has witnessed many a cautionary tale, where a company’s A/B test results have run it off the rails. __“Abdicating product vision is a very common danger with A/B testing,”__ says Ries. He recalls talking with an ex-employee of a company that went under. Ries asked the former employee what went wrong, and the employee explained, “Yeah, if you’re on a social platform, and if all you do is A/B test all the time then you immediately get yourself into sexting, porn, and nasty stuff because that’s what converts.”
>
> Not every company finds itself falling into quite such a dramatic identity crisis, but the broader point is one that almost any organization can relate to: how to balance customer feedback and brand identity. For example, an e-commerce site may find that displaying sale prices bigger and with big red strike-through lines improves their conversions. But that increase in conversions could cost them something potentially more valuable: their brand. The site may begin to look like a discounter when it’s really a boutique.
>
> “It comes down to understanding who you are,” says Chrome Industries e-commerce Director Kyle Duford. “Just because you can doesn’t mean you should.”
>
> Many companies are extremely particular about their brand image to the point that they are not willing to test things that they wouldn’t put live on their site. It’s sound advice. A question worth asking about every test is, “Would you be happy showing the winning variation to all of your traffic?” If not, then what exactly are you hoping to gain from the test?


There are several valid answers to this question, and it’s worth pointing out that many companies do in fact experiment with things they wouldn’t necessarily be committed to rolling out at full scale. 

> One such company is Dell, where Marketing Director Ed Wu argues that there can be a place for testing variants of a page that the company isn’t interested in implementing in the near-term.
>
> Using A/B testing to learn about how visitors engage with certain types of content or color or messaging can be illuminating and can feed back into later conversations with Dell’s brand team or the global site design team. “Our global design team has very consistent and very stringent guidelines in terms of what color you can use on dell.com,” says Wu. The global guidelines for dell.com call for a blue banner of a specific hue at the bottom of the page, and Dell’s testing team was interested in pushing back on that guideline to see whether that blue is in fact the best color to have on the site.
>
> So Wu’s testing team worked with Dell’s branding team and global design team to come up with some alternatives, including a bottom banner that was red. “[We] know that it’s violating our brand standard, we would not be able to implement that, but eventually we’ll all agree, let’s go ahead and test it and understand how it works,” says Wu. Testing for learning and understanding, not just conversions, allows Dell to “push the thinking” on what is optimal for the site.

---

> TL;DR
> - A/Btestingcanhelpeventhesmallestoforganizationsand sites, but it becomes more powerful the more traffic a site has. If your site is starved for traffic, A/B testing probably shouldn’t be your first priority.
> - Testingwilloccasionallyrevealachangethatincreasesone metric but decreases another. Don’t push failure down the funnel.
> - Considerwhetheryouarewillingtotestapagevariantthat is in some way off-brand or one that you wouldn’t necessarily be quick to roll out to all your users should it “win” the test.

---

## Chapter 12 - Beyond the Page: Non-Website A/B Testing - How to Test Email and Pricing

### The What and the When: Prezi

Email is one of the most important things you can A/B test. 

Just as you can use A/B testing to make the elements of your website work better, you can A/B test emails to increase quantifiable success metrics like open rate and click-through rate. And just as you can roll out a website change gradually and gauge response before showing it to all users, it’s easy and incredibly advantageous to roll out an email in the same way. 

First, select a portion of your total mailing list and send them a number of variations. Gauge your success metrics, and then send the email that works best to the remainder of your full list.

> It was Halloween of 2011, and David Malpass — a marketing analyst at cloud-based presentation tool Prezi — was planning to send out a huge email newsletter blast. He sent his boss, the director of marketing, three candidate subject lines:
>
> 1. “New Tricks & Treats”
> 2. “New features: Templates, Google Image Search”
> 3. “Create a beautiful Prezi in minutes”
>
> Malpass recalls his boss’s response: “Why don’t you try one that just says ‘Boo!’?” Malpass thought it was a terrible idea.
>
> Malpass was planning to A/B test the subject line anyway, so he reluctantly threw “Boo!” into the mix and sent out emails with the four different subject lines to about a million users. He remembers the results with a smile: “Boo!” trounced the other three subject lines with a whopping 20 percent more opens.
>
> Email also offers some intriguing opportunities for tests that don’t have anything to do with either the content or the formatting. Namely, because email (unlike a website) actively reaches users (instead of passively awaiting their arrival), the timing of the message can itself be tested.
>
> As it turns out, the hour of the day and the day of the week can both matter in a big way. Educational licenses make up a large portion of Prezi’s user base, and Malpass was in for a surprise when he tested for the day of the week with the optimal open rate. He got not one answer, but two: for educators, Monday had the highest open rate, and the rate decreased through the week. For non-educators, open rate peaked late in the week, on Thursday.

### Price Testing

There’s another pivotal element to every web business that’s asking to be tested, beyond the layout and media and copy and even newsletter timing, and arguably even more important: __the price__. Price testing is a very valuable way to understand how demand for your product changes with price increases and decreases.

> “Price testing is some of the most valuable testing and some of the most challenging technically,” explains Scott Zakrajsek, who has done testing at e-commerce sites like Adidas, Victoria’s Secret, and Staples. “If you’re a smaller site with ten to twenty key products, then price testing is the number-one thing you can do to maximize your conversion, your revenue.”
>
> The vast majority of prices today aren’t really well thought through. It’s easy to determine cost-plus pricing, where you take your own costs of production and add some kind of profit margin, as well as competitive or market-based pricing, but much trickier to come up with value-based pricing—getting the price in line with the value that you deliver.
>
> With A/B testing, conventional wisdom and generalizations surrounding pricing become obsolete. There are many services out there where if you increase the price, just as many people will buy it. In some rare cases, and especially if your customers are buying something on behalf of their company, they are more likely to buy if the price is higher. Your customers might ask: if other businesses are paying this much for it, it must be good, right? Every product’s and service’s demand curve will look different, and testing prices is one of the best ways to get data about your own.
>
> There’s just as much theory and anecdote surrounding a price’s trailing digits: the cents. 
>
> Some people argue that round-dollar figures connote a premium product (and have design possibilities that include eliminating the decimal portion altogether);
>
> others argue that there is an important lure to prices that end in 99¢. 
>
> Some authorities will argue the differences between 95¢, 96¢, 97¢, and 98¢; 
>
> Walmart is known for its seemingly random cent-pricing ($6.37 toothpaste, $8.02 stapler), which seems to connote that prices have been cut to the bone. 
>
> Where does that leave you and your business?

What’s interesting is that these are all psychological tactics that are very difficult to understand fully or to generalize from another business’s results to your own. By A/B testing them you can very quickly measure and see for yourself exactly how those factors are in play with your own business. You can plot your own price elasticity curve, and in some cases you’ll see that if you increase the price, consumption stays the same. The only way to know for certain is to test it.

### Testing the Perceived Price: The Last-Minute Discount

There are two hurdles to successful price testing. 

The first is technical: most prices are stored in a database or pricing table, and so any test that modifies the price of an item is going to need much deeper technical integration than a test simply involving only front-end design elements. 

The second hurdle is about managing customer relationships: to accidentally show users a lower price on the page and then charge them a higher price breaks not only customers’ trust but the law. And users who are charged a higher price even if the site is transparent about it may feel betrayed or alienated when they learn that most other customers paid less.

There is a quick-and-easy method to foray into price testing that avoids all of these problems, something we call the last-minute discount. The way it works is quite simple. You show the customer a higher price (say $13.99) than the “actual” price of that item, which most users see (say $9.99) and which lives in the back-end price book. As the user in the $13.99 variation group moves down the checkout funnel, they discover just before confirming their purchase that a “last-minute discount” has been applied and that their total is in fact only $9.99! You’ve managed to get reasonably accurate information about what kind of conversion rate to expect from the higher price without dealing with back-end integration or worrying about making customers upset.

This type of cosmetic price testing is also perfect for testing the cents of a price. A product shown for $19.63, $19.95, and $19.99 in different tests could check out at $19.50 or $19.00 even; the users may not even notice the change. (Just be sure not to charge customers more than they’re expecting!) Reading the conventional wisdom about cent pricing yields various — often mutually exclusive — theories and superstitions. The truth is that every business is different; you won’t know until you test.

### Anchoring in Action: Judy’s Book Club

Another way to get data on your pricing without literally offering a product for different prices to different users is to test the way that product’s price is anchored, or contextualized, for the customer. 

Consumers typically don’t buy the most expensive options available, and many times they don’t buy the least expensive, either. (Restaurants, for instance, will often mark up the second-least-expensive menu option.) Perhaps displaying only paid plans on your site will get users in at the lowest rung; perhaps offering a “Free” plan with virtually no features will help persuade users that adequate services can’t be had for free. Try both.

> Judy’s Book is a “social search” tool and reviews website that caters to a family audience. Paid business listings are a primary way Judy’s Book makes money: businesses get better search positioning and more robust profiles with photos if they pay a monthly fee.
>
> Judy’s Book wanted to increase the number of businesses that sign up for paid listings, and General Manager Ali Alami hypothesized that positioning a column showing the few features included with a free listing alongside the many features included in the paid listings would increase signups for paid listings.
>
> Showing a free listing column increased clicks on the signup button for the basic paid listing by 198.6 percent.
>
> There are a number of additional anchoring techniques worth considering, and, of course, testing. 
>
> For instance, try adding a tier above your most expensive tier in order to make the one beneath it seem less expensive. And try having a small price delta between the plan you want people to buy and the one just below it, with the goal being to get customers to think they’re getting a bargain (“For just a little bit more, I get all this stuff!”).

### Testing the Billing

You have options when it comes to how the price is broken down to the user as well. You might test representing the price as an annual subscription price or as a monthly cost. You might, for instance, have a monthly price prominently displayed but explain to the user that the product is in fact billed annually. (Just be sure it’s clear how much customers will be billed so they don’t get a surprise charge.) Testing will reveal how these differences affect conversion, average order value, churn, and so forth.

In testing their own pricing display, for instance, Prezi saw a 12.5 percent increase in signups from emphasizing the monthly breakdown of its annual cost.

It’s worth keeping localization effects in mind here: there are markets like Brazil, for instance, where appliances and hardware are more typically bought with monthly payments than they are in countries like the United States. (Laws can also differ from country to country in ways that affect pricing.) Understanding your market will help you get your bearings, but as always, test and see what works best.

### Testing the Actual Price


Changing the actual price of a good or a service is the most complex of these approaches, in that it requires you to actually change your price book or to add additional SKUs. Full-fledged price testing has a world of nuances and best practices all its own. 

For instance, perhaps serial testing across all users at different times will avoid potential PR fallout rather than having different prices in play simultaneously. This method has its drawbacks, though: for instance, it becomes difficult to control for the general outside dynamics. One reason why you typically do A/B testing over different users in the same time period is to control for time-based effects, like day-of-week, time-of-day, news cycle, and the like. Typically, making meaningful sense of serialized price changes requires historical data that can put small-scale fluctuations into context.

A quick final reminder: it’s absolutely critical when price testing to make sure you are defining your success metrics correctly. A higher price is very likely going to reduce conversions, but increase average order value: the key question is by how much? Make sure that you’re using an appropriate metric, for instance, revenue per visitor (RPV), which takes into account both conversion and average order value, in order to get accurate reporting from your testing tool about which variation is the winner. Consider, also, that even RPV may not tell the whole story. Higher pricing may lead to customer churn in the medium-term, so think carefully about what your results truly mean before making a major change.

---

> TL;DR
> - Not only the subject lines but the timing can be critical in influencing open rates and click-through rates for email. Roll out large email campaigns gradually, test variations, and then send the winner on to the rest of your list.
> - Price testing can be one of the most important types of testing to conduct for your business, and there are a number of ways to do it.
> - Thelast-minutediscountisagreattechniquefortesting price without needing to deal with back-end integration or worrying about upsetting users.
> - Anchoring the price of a product into context with other prices can greatly affect how users react.
> - Presentationcanbeeverythingwhenitcomestopricing. For a quick-and-easy price test, try various breakdowns (e.g., monthly, yearly, or weekly) and see what works best.
> - Serial testing is one way to test prices without needing to show different users different prices at the same time; however, this advantage is offset by difficulties in ensuring the accuracy of its results.

---

## Chapter 13 - Personalize, Personalize, Personalize - Moving Beyond the One-to-Many Web

### Targeting versus Segmentation

There are two things that Optimizely and other testing solutions enable to help sites make sense of their user populations: targeting and segmentation. 

Targeting happens before the test and is essentially the definition of who is allowed to see a particular experiment, based on the URL and any number of conditions. One cohort of visitors (say those who come to your site via social media) will see one variation while visitors coming from search engines will see a different one.

Segmentation happens after the test and takes a different approach: you run the experiment for everybody and then isolate different groups of segments afterward and figure out how each performed.

A great example of where segmentation can be handy is with mobile and tablet browsers. Perhaps your proposed new page layout works great overall: you still want to make sure that it’s not a regression or an inferior experience for users arriving on smartphones or tablets. For instance, perhaps a button element is too small to be comfortably clicked on a mobile screen: this would likely show up in a segmentation report, where the conversion and engagement of mobile users is lagging behind that of their desktop counterparts.

A growing number of websites are moving from providing one-to-many, average best experiences for all visitors to one-to-few experiences that involve smart targeting based on browser, location, behavior, and more. The final step in this evolution is the one-to-one web: a personal experience tailored for each individual user. It’s easy to imagine that several years from now people will look back and be shocked at how generic, impersonal, and one-to-many the web we know today is. It’s inevitable that the web will move toward a more personalized one-to-one experience and it’s just a question of how that reality will come about.

### Using Segmentation to Drill Down into Test Results

Imagine that a company tries a new site layout and notices that overall sales increase. A slightly more detailed analysis might reveal that the sales decreased slightly for people who visited the site on tablet devices. The company then starts hypothesizing about why that happened and what was different for this visitor segment. The issue could be, for instance, that the new design pushed the “Add to Cart” button below the fold on an iPad and the user had to scroll down to tap it. That’s a good hypothesis about what could be causing sales to drop, and a perfect place to begin a second experiment. The company targets a segment or audience (perhaps just iPad users) and tries to deliver an even better experience to them.

The natural testing cycle is usually to start off by examining the one-to-many results data: how did all website visitors react to the test? Then, slicing the data into granular pieces based on any number of conditions (UTM source, browser, cookies, referral URL, location) generates specific questions like, how did paid traffic react to the test? Results from specific segments in one experiment turn into the targeting criteria for the next experiment. Iterative loop closed!

### Geo-Targeting, State by State: Romney 2012

> From the start, the digital campaign team for Mitt Romney’s 2012 presidential campaign considered increasing email signups on mittromney.com to be one of their primary goals. As Ryan Meerstein — a senior political analyst from Targeted Victory who ran testing and optimization for the Romney campaign — explains, “Email is still the golden goose of fundraising when you’re making direct solicitations. We’re seeing each email valued at anywhere between seven and eight dollars in future revenue.”
>
> Between May 2011 and November 2012, the Romney campaign’s 140-person digital team along with Targeted Victory ran hundreds of tests. “Once we saw [how easy it was to conduct A/B testing], the ideas started flying. We wanted to start testing just about everything,” Meerstein says. “We started on the splash page and when we saw success, we continued to build from there.”
>
> They tried showing different landing pages to visitors from different states, hypothesizing that visitors would sign up for email updates more if they saw a message specific to their state. They tested the state-specific geo-targeted message against a universal landing page lacking any state-specific messaging.
>
> They found that when they simply added the state name to the call to action text, visitors entered their email and zip code 19 percent more often.
>
> Starting in September 2012 and lasting until Election Day, visitors to mittromney.com received a distinct experience depending on their home state, which proved to be a valuable tool for the campaign. The data clearly showed that personalizing the message led to success. With this test as testament, the team decided to make the splash page specific for each state. They used geo-targeting to send visitors from each state to a page with a message specific to that state. They also crafted personalized calls to action based on absentee-vote states and early-vote states. Visitors from Ohio saw messages directing them to early voting locations; visitors from Colorado saw targeted messages for how to get an absentee ballot.
>
> As a result, the Romney team saw not only greater signups on the splash page but more interaction with local events advertised on the site, especially in the critical hours after voting started. “The thing that was great about it was that we could go [to our A/B testing tool] and set up the personalized experiences in thirty minutes,” Meerstein says. “In the final weeks of the campaign, there’s a huge difference between something being live on Tuesday morning and Thursday night.”

### When to Personalize and When Not to: Wikipedia

> Wikipedia is a useful case study in the art of not tailoring the experience to every user. For instance, when Wikipedia’s annual fundraising push comes around, the site is able to know that one user may be coming from Toronto for information about film directors, and another from Sydney for information about world history, but it shows both users (and all users) the very same message. That’s a deliberate choice, and it also happens to be the correct one. How do they know this? They’ve tested it.
>
> Through A/B testing, the fundraising team discovered that their users appear to be less likely to donate when faced with a targeted fundraising appeal than with a universal one. It could be that users find personalization intrusive, or it could be that part of Wikipedia’s brand is its openness and universality.
>
> The Wikipedia example illustrates that despite its incredible power as a tool, personalization isn’t always going to be more effective. Sometimes the “average best” is in fact simply “best.” The web is not a place for static universal truths, and personalization is no exception. How is a company to know whether personalization will take its success metrics to the next level, or set them back? There is only one answer: test it.

---

> TL;DR
> - Segmentation allows you to compare how different segments of users responded to the same experience. Differ- ences that you observe between groups can be illuminating and give you an opportunity to go beyond the average best experience of the one-to-many web toward an improved one-to-few experience.
> - Targeting is deliberately providing different types of users with different experiences, and can be a powerful technique under many circumstances.
> - Consider how your user experience may or may not be optimized across different platforms, screen dimensions, touch input versus mouse input, and so on.
> - Sometimes geo-targeting, or tailoring users’ experience based on their location, can be an extremely powerful way to optimize beyond the “average best.”
> - While personalization is frequently a boon for your suc- cess metrics, occasionally a universal call to action works better than a targeted one. Test to make sure personalization is working for your key success metrics.

---

# Appendix 1 - 60 Things to A/B Test

## Calls to Action

Your website exists for visitors to take action: reading, purchasing, signing up, downloading, or sharing. Here are some ways to test calls to action that can yield quick, easy, and big wins.

1. Buy now? Purchase? Check out? Add to cart? Change the call to action (CTA) text on your buttons to see which word or phrase converts more visitors.

2. Try varying the location of your CTA button, making some CTAs more prominent than others.

3. Test multiple CTAs per page against one CTA per page.

4. Change buttons to hyperlinks to find out which display your users prefer.

5. Find out if CTAs with text, icons, or text plus icons convert more users on your site.

6. Test different colors, shapes, and sizes for CTA buttons on your website.

## Content

Content fuels your online business, particularly if you’re a B2B company. Testing how you position and advertise content on your site can uncover big conversion and engagement lifts.

7. Test gated content against nongated content.Find out if your users are willing to sign up or provide more information to access materials on your site.

8. Do site visitors crave more information about your company before signing up or making a purchase? Test adding or removing “About” content on your home page.

9. Content tone can make a big difference in keeping users on your site. See what your visitors prefer by testing various tones and styles.

10. Test how your content is displayed. Do users prefer to scroll down the page or click through to another page to learn more?

## Copy

Copy is your direct line of communication with website visitors — it sets the tone and helps users understand what you’re all about. Use these tests to make the copy on your site better resonate with your audience.


11. Test different headline texts. Try variations that are straightforward against ones that are abstract, goofy, or creative.

12. Find out if your site visitors prefer shorter versions of headlines, taglines, product descriptions, and other content on your site.

13. Run a multivariate test. Test different combinations of headlines and taglines in combination with the visual imagery on your page to find the ultimate winning variation.

14. Test paragraphs versus bulleted lists.

15. Test how you frame your copy. Users may have different reactions to positive versus negative messaging.

16. Try making your site easier to read with larger fonts, highercontrast colors, and professional fonts (not Comic Sans). Studies show this increases trustworthiness and increases conversions.

### Visual Media

Digital media have the power to greatly influence conversions and engagement on a website, and testing digital media is a great idea because the right media can subconsciously influence people to act in a way that’s aligned with your testing goals.

17. Test different types of images on your landing page. People versus products is a good place to start.

18. And iterate from there! Try a static image versus a product video versus a 360 degree product image.

19. See how a stock image stacks up against an image of your employees or customers in action.

20. Test a rotating carousel on your home page versus a static image or video.

21. Test different voice-overs for the videos on your site. Test whether a male or a female voice leads to the most completed views.

22. Try different variations of your site’s product demo:animated versus screencast.

23. Test auto-play against click-to-play video.

### Funnels

If your goal is to get more people from one page to the next — like in a checkout funnel, signup flow, or lead nurture — then A/B testing is your best bet. Funnels are rife with low-hanging fruit to test.

24. Test removing extraneous distractions — like product offers, promotions, or shipping information — from each page in the purchase flow. Oftentimes a simplified experience can drive more conversions.

25. Test the number of pages in your funnel. How does packing more information on one page compare to spreading information across multiple pages?

26. Test removing navigation to any pages outside the checkout funnel.

27. Or try replacing certain steps within your funnel with modal boxes. For example, try making shipping options a modal box instead of a page.

### Site Navigation

From the moment a visitor lands on your site, the navigation menu sets a foundation — it’s how people maneuver your site’s flow and prioritize what’s important. Here are some ideas for how to make it better:

28. Test the order of menu items in your site navigation.

29. Test the display of your navigation bar. Do site visitors prefer a horizontal or vertical orientation?

30. Or what about a fixed navigation bar that travels down the page as your site visitors scroll?

31. Test out the title of your navigation items. A simple change, like “Why Use Us” to “How It Works,” may have a significant impact.

32. Testing Tip: If a test fails, try targeting the test to new versus returning visitors. Returning visitors are accustomed to seeing the site in a certain way — if a link or menu item is missing from the spot they normally go to in order to find it, they’re not going to do the work to locate it.

### Forms

Any potential friction point on a website is prime for testing. Forms are frequently cumbersome areas of websites. Try these tests on the forms on your site:

33. Test the length of signup forms. Try removing nonessential signup boxes or relocating them to a page further down the funnel.

34. Try a special offer, discount, or promotion to increase sign-ups. People love free stuff.

35. Spam is the worst. Try adding text that assures users you won’t fill up their inboxes with unnecessary junk.

36. Try making individual form fields larger. Larger fields feel more friendly.

37. Try asking for different information in your form fields — for example, business email versus regular email, or work phone versus mobile phone.

### Mobile Site

The mobile web is pervasive. Improving your mobile website through testing will help create an optimized experience that generates more click-throughs, revenue, and conversions.

38. Test the length of your mobile web pages. Are mobile users more willing to click to a new page or scroll down a page when browsing your site on their devices?

39. Try different displays and navigation options. Blinds, buttons, and blocks are good places to start.

_Testing Tip_: When testing your mobile website, try targeting mobile users based on their operating system — Android or iOS, for example — to learn more about your mobile website visitors.

### Advertising

A/B testing increases the value of the money you’re already spending on marketing programs, such as search engine marketing. To ensure you’re getting the biggest bang for your buck out of each paid initiative, try these tests:

40. Test the headlines on your paid campaigns to see which ones get the most clicks.

41. Try changing up the display URL on your ads. This can impact how many visitors click the ad.

42. The landing page each ad directs to is an excellent place for testing. You paid for that visitor to land there, so you want to do everything you can to convert that visitor.

### Social


The reasons someone would share your site are many — make it easy for them to do so. Here are a few tests to increase likes, retweets, and þ1s on your content:

43. Change the size and placement of social icons to see what compels users to share more often.

44. Test standard social media icons against ones you’ve designed to match the look and feel of your site.

45. Try finding your optimal Twitter voice. Tweet the same link with different types of text at the same time two days in a row and see which tone of voice gets more engagement.

46. Test different types of customer reviews on your site to see which are most compelling to your audience. Some examples include testimonials, Yelp reviews, and ResellerRatings.

### Email

How do you make sure your marketing emails get opened and, dare we say, clicked? Here are some testable elements that can increase open rates and click-throughs:

47. Test length and copy of your email subject lines.

48. Test personalized versus unpersonalized emails by using the recipient’s name in the subject or email text.

49. Find the optimal time to reach your audience by measuring open rates on different days of the week and at different times of the day.

50. If you distribute a newsletter or email update, see how a weekly send stacks up against a monthly blast.

51. Would readers prefer an email from your CEO, your marketing director, your broader team, or someone else? Test different “from” addresses to find out.

### Personalize It

Today, we’re more accustomed to web experiences that are custom-tailored to who we are and the URLs we’ve come from. Try testing these personalization techniques and see if visitors convert better.

52. Create seasonal or holiday-specific promotional offers and test them on visitors living in specific locations.

53. Test auto-filling form fields related to a site visitor’s location. Make things easier for your users.

54. Test matching language-specific content to users coming from a certain country or region.

55. Test different page designs and messaging for new versus returning visitors.

56. Test whether showing different landing pages for visitors coming from mobile devices versus desktop browsers performs better than having the same landing page for both.

### Pricing and Shipping

Finding the right pricing point can drive more visitors to make a purchase. Use these tests to maximize revenue from your site:

57. Test offering a free trial versus a money-back guarantee to see which converts more users in the short term and in the long term.

58. Test having checkboxes auto-selected as default (such as billing information being the same as shipping info).

59. On your pricing page, test whether annual billing or monthly billing generates more subscriptions.

60. Try anchoring customers high before showing them a lower price. For example, “Competitors charge $2.9 trillion, but you can use us for just $2.99!”

# Appendix 2 - Metrics and the Statistics behind A/B Testing

## Confidence Intervals

Suppose we know that 51.4 percent of the population of the City of San Francisco has a bachelor’s degree or higher. If we were to choose 1,000 city residents at random, we’d expect that exactly 514 of those people would have a bachelor’s degree or higher. In reality, of course, this rarely happens. Why not? First, depending on your sample size, it may not be mathematically possible to arrive at exactly 51.4 percent (try this example with a sample size of 100 instead of 1,000). Second (and more important), by using a small sample to represent a large population, we are introducing some error.

In reality, it’s usually difficult or impossible to measure the exact value of a statistic for an entire population; hence the obvious value of sampling. It seems, then, that we need a way to quantify the reliability of our sample data. We do this using estimates.

When we talk about statistics from a sample, we tend to provide two types of estimates: 

- point estimates (single numbers) and 
- interval estimates (two numbers). 

If we were to poll 1,000 city residents chosen randomly, and found that 509 respondents had earned a bachelor’s degree or higher, our point estimate would be 50.9 percent (509/1,000). The interval estimate is slightly more complex and depends partly on how certain we need to be in our estimate. The latter, often called the desired confidence level, varies by application, but for most A/B testing and other business analytics in general, __95 percent confidence__ is the standard.

For the time being, let’s assume a 95 percent desired confi- dence level. The formula for the 95 percent interval estimate is given by:

p +- 1.96 * sqrt((p * (1-p)) / n)

- p: Our __point estimate__ (50.9% or 0.509)
- 1.96: A normal curve Z-value estimate corresponding to __95% significance__
- n: Our __sample size__ (1,000)

For our example, the interval estimate would be: 0.509 +- 0.031

We interpret that by saying we are 95 percent confident that the rate of San Francisco residents having a bachelor’s degree or better is between 47.8 and 54.0 percent.

---

If we are instead interested in the average age of a San Francisco resident, the approach is the same and the formula very similar.

x +- 1.96 * sqrt(s^2 / n)

- x: The __average__ age from our sample
- n: Our __sample size__
- s^2: The __variance__ in age from our sample

If your data is in Excel, you can compute the sample mean and variance by using the average() and var() formulas, respectively.

---

For both the average age and the percentage of degree holders measurements, there are three conditions that result in a large confidence interval:

- High variance in our data (inconsistent data)
- A small sample (not enough data points)
- A high desired confidence (greater than 95 percent)

In other words, to reduce the size of the interval, we’ll need to 

- take a larger sample
- select a metric with less variability
- accept lower confidence in our results

## Confidence Levels


As mentioned earlier, the standard confidence level for most business applications is 95 percent. Other than its pervasiveness, there is no reason statistical studies need to be limited to 95 percent. In other applications, such as biomedical research, manufacturing, or statistical computing, it is not uncommon to use other confidence levels (80 percent, 90 percent, and 99 percent are other common ones).

So what does a confidence level actually mean, anyway? Recall that earlier we said a 95 percent confidence level indicates we are 95 percent confident the true population value lies between 47.8 percent and 54.0 percent. In that example, we happened to know the true value was 51.4 percent, so our interval was correct. What a 95 percent confidence level actually tells us is that if we repeated the survey many, many times and computed a confidence interval for each, 95 percent of those confidence intervals would contain the true population value.

In practice, we don’t take multiple samples; we take one or two. For that reason, it becomes important to select the right metric and ensure that the sample size is sufficiently large to detect differences in the key performance indicator (KPI) of interest.

## A/B Testing Metrics Framework

The preceding examples all dealt with a single sample. In A/B testing, however, we are interested in comparing multiple samples. For purposes of simplicity, here we will focus on two-sample comparisons (a simple A/B paradigm).

Suppose we wanted to compare conversion for two variants: current versus test. Using the principles applied earlier, we could derive point and interval estimates for each of the two versions.

TODO finish section

TODO ### The Z Statistic for Two Proportions
TODO ### The t Statistic for Two Averages

